[
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html",
    "title": "“ImportError: DLL load failed while importing cv2: The specified module could not be found.”",
    "section": "",
    "text": "If your on Windows using python &gt;= 3.8 having built OpenCV &gt;= 4.6 from source and your seeing the above error when calling import cv2, this short guide should help solve your problem.\nThe guide assumes that you have either installed the python bindings during the build process (not recommended) or manually copied cv2.cp3x-win_amd64.pyd to your distributions site-packages directory (e.g. C:\\Users\\&lt;USER&gt;\\mambaforge\\Lib\\site-packages).\nSo what’s the issue? Although the message is quite explicit regarding the cause, it doesn’t really help with finding a solution. In a nutshell python has found cv2.cp3x-win_amd64.pyd, tried and then failed to load it because a it can’t find a dependant shared library. Now the advice I have seen online regarding this is to dig out trusty dependancy walker, load the cv2.cp3x-win_amd64.pyd and see which dependencies the system can’t find.\nNow this is solid advice if we had a C++ application and/or we were using python &lt; 3.8 (which uses the system/user path for dll resolution), however we are not, so even if dependency walker can’t detect any problems we may still be facing the above error.\nThe good news is there is an easy fix if you know where the missing DLL’s are and only slightly more involved if you don’t as long as you have access to the missing DLL’s on your system."
  },
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#fix-when-path-to-missing-dlls-is-known",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#fix-when-path-to-missing-dlls-is-known",
    "title": "“ImportError: DLL load failed while importing cv2: The specified module could not be found.”",
    "section": "Fix when path to missing DLL’s is known",
    "text": "Fix when path to missing DLL’s is known\nTo demonstrate the fix, I have built the OpenCV shared library and corresponding python bindings and manually copied them to the site-packages directory inside my python distribution (C:\\Users\\b\\mambaforge\\Lib\\site-packages).\nAs I have built a shared library the python bindings are dependant on opencv_world470.dll and I haven’t told python where they are I get the error shown below whenn trying to import them.\n\nimport cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\nGiven that I know the path to OpenCV’s shared libraries is required and I haven’t told python about it, the first thing to try is to add it to pythons DLL search path and see if that solves the problem.\n\nimport os\n#os.add_dll_directory(\"D:\\\\build\\\\opencv\\\\4_7_0\\\\cuda_12_D\\\\bin\")\nos.add_dll_directory(\"D:\\\\build\\\\opencv\\\\4_7_0\\\\cuda_12_0_cc_6_1_8_6_ff_scratch_D\\\\bin\")\nimport cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\nAhh the same error, what’s going on.\nIn this case I also built OpenCV against the CUDA SDK so there is a good chance its missing DLL’s from there aswell. I can try to fix the issue by simply adding the location of the CUDA SDK binaries to the python DLL search path as shown below.\n\nos.add_dll_directory(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.0\\\\bin\")\nimport cv2 as cv\n\n\nnpMat = (np.random.random((129, 128, 3)) * 255).astype(np.uint8)\ncuMat = cv.cuda_GpuMat()\ncuMat.upload(npMat)\nassert(cuMat.cudaPtr() != 0)\ncuMatFromPtrSz = cv.cuda.createGpuMatFromCudaMemory(cuMat.size(),cuMat.type(),cuMat.cudaPtr(), cuMat.step)\nassert(cuMat.cudaPtr() == cuMatFromPtrSz.cudaPtr())\ncuMatFromPtrRc = cv.cuda.createGpuMatFromCudaMemory(cuMat.size()[1],cuMat.size()[0],cuMat.type(),cuMat.cudaPtr(), cuMat.step)\nassert(cuMat.cudaPtr() == cuMatFromPtrRc.cudaPtr())\nstream = cv.cuda_Stream()\nassert(stream.cudaPtr() != 0)\nstreamFromPtr = cv.cuda.wrapStream(stream.cudaPtr())\nassert(stream.cudaPtr() == streamFromPtr.cudaPtr())\nasyncstream = cv.cuda_Stream(1)  # cudaStreamNonBlocking\nassert(asyncstream.cudaPtr() != 0)\n\n\ncuMatFromPtrRc.size()\n\n(129, 128)\n\n\n\ncv.cuda_GpuMat(1,10,0).size()\n\n(10, 1)\n\n\n\ncuMat.size()\n\n(128, 129)\n\n\n\nhelp(cv2.cuda.wrapStream)\n\nHelp on built-in function wrapStream:\n\nwrapStream(...)\n    wrapStream(cudaStreamMemoryAddress) -&gt; retval\n    .   @brief Bindings overload to create a Stream object from the address stored in an existing CUDA Runtime API stream pointer (cudaStream_t).\n    .   \n    .   @param cudaStreamMemoryAddress Memory address stored in a CUDA Runtime API stream pointer (cudaStream_t). The created Stream object does not perform any allocation or deallocation and simply wraps existing raw CUDA Runtime API stream pointer.\n    .   \n    .   @note Overloaded for generation of bindings only, not exported or intended for use internally from C++.\n\n\n\nThis appears to have solved the issue, but its a good idea to examine the build information just to double check I have loaded the right version of OpenCV.\n\nprint(cv2.getBuildInformation())\n\n\nGeneral configuration for OpenCV 4.7.0 =====================================\n  Version control:               4.7.0\n\n  Extra modules:\n    Location (extra):            D:/repos/opencv/contrib/modules\n    Version control (extra):     4.7.0\n\n  Platform:\n    Timestamp:                   2023-01-31T15:00:40Z\n    Host:                        Windows 10.0.22621 AMD64\n    CMake:                       3.25.1\n    CMake generator:             Ninja\n    CMake build tool:            D:/bin/ninja/ninja.exe\n    MSVC:                        1934\n    Configuration:               Debug\n\n  CPU/HW features:\n    Baseline:                    SSE SSE2 SSE3\n      requested:                 SSE3\n    Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX\n      requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n      SSE4_1 (18 files):         + SSSE3 SSE4_1\n      SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n      FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n      AVX (5 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n      AVX2 (34 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n      AVX512_SKX (8 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX\n\n  C/C++:\n    Built as dynamic libs?:      YES\n    C++ standard:                11\n    C++ Compiler:                C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.34.31933/bin/Hostx64/x64/cl.exe  (ver 19.34.31937.0)\n    C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819  /MD /O2 /Ob2 /DNDEBUG \n    C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819  /MDd /Zi /Ob0 /Od /RTC1 \n    C Compiler:                  C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.34.31933/bin/Hostx64/x64/cl.exe\n    C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS       /MD /O2 /Ob2 /DNDEBUG \n    C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS     /MDd /Zi /Ob0 /Od /RTC1 \n    Linker flags (Release):      /machine:x64  /INCREMENTAL:NO \n    Linker flags (Debug):        /machine:x64  /debug /INCREMENTAL \n    ccache:                      NO\n    Precompiled headers:         NO\n    Extra dependencies:          cudart_static.lib nppc.lib nppial.lib nppicc.lib nppidei.lib nppif.lib nppig.lib nppim.lib nppist.lib nppisu.lib nppitc.lib npps.lib cublas.lib cudnn.lib cufft.lib -LIBPATH:\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.0/lib/x64\"\n    3rdparty dependencies:\n\n  OpenCV modules:\n    To be built:                 aruco barcode bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab wechat_qrcode world xfeatures2d ximgproc xobjdetect xphoto\n    Disabled:                    -\n    Disabled by dependency:      -\n    Unavailable:                 alphamat cvv freetype hdf java julia matlab ovis python2 python2 sfm viz\n    Applications:                tests perf_tests examples apps\n    Documentation:               NO\n    Non-free algorithms:         NO\n\n  Windows RT support:            NO\n\n  GUI: \n    Win32 UI:                    YES\n    OpenGL support:              YES (opengl32 glu32)\n    VTK support:                 NO\n\n  Media I/O: \n    ZLib:                        build (ver 1.2.13)\n    JPEG:                        build-libjpeg-turbo (ver 2.1.3-62)\n      SIMD Support Request:      YES\n      SIMD Support:              NO\n    WEBP:                        build (ver encoder: 0x020f)\n    PNG:                         build (ver 1.6.37)\n    TIFF:                        build (ver 42 - 4.2.0)\n    JPEG 2000:                   build (ver 2.4.0)\n    OpenEXR:                     build (ver 2.3.0)\n    HDR:                         YES\n    SUNRASTER:                   YES\n    PXM:                         YES\n    PFM:                         YES\n\n  Video I/O:\n    DC1394:                      NO\n    FFMPEG:                      YES (prebuilt binaries)\n      avcodec:                   YES (58.134.100)\n      avformat:                  YES (58.76.100)\n      avutil:                    YES (56.70.100)\n      swscale:                   YES (5.9.100)\n      avresample:                YES (4.0.0)\n    GStreamer:                   NO\n    DirectShow:                  YES\n    Media Foundation:            YES\n      DXVA:                      YES\n\n  Parallel framework:            Concurrency\n\n  Trace:                         YES (with Intel ITT)\n\n  Other third-party libraries:\n    Intel IPP:                   2020.0.0 Gold [2020.0.0]\n           at:                   D:/build/opencv/4_7_0/cuda_12_D/3rdparty/ippicv/ippicv_win/icv\n    Intel IPP IW:                sources (2020.0.0)\n              at:                D:/build/opencv/4_7_0/cuda_12_D/3rdparty/ippicv/ippicv_win/iw\n    Lapack:                      NO\n    Eigen:                       NO\n    Custom HAL:                  NO\n    Protobuf:                    build (3.19.1)\n\n  NVIDIA CUDA:                   YES (ver 12.0, CUFFT CUBLAS NVCUVID NVCUVENC FAST_MATH)\n    NVIDIA GPU arch:             50 52 60 61 70 75 80 86 89 90\n    NVIDIA PTX archs:            90\n\n  cuDNN:                         YES (ver 8.7.0)\n\n  OpenCL:                        YES (NVD3D11)\n    Include path:                D:/repos/opencv/opencv/3rdparty/include/opencl/1.2\n    Link libraries:              Dynamic load\n\n  Python 3:\n    Interpreter:                 C:/Users/b/mambaforge/python.exe (ver 3.9.13)\n    Libraries:                   C:/Users/b/mambaforge//libs/python39.lib (ver 3.9.13)\n    numpy:                       C:/Users/b/mambaforge/Lib/site-packages/numpy/core/include (ver 1.23.5)\n    install path:                C:/Users/b/mambaforge/Lib/site-packages/cv2/python-3.9\n\n  Python (for build):            C:/Users/b/mambaforge/python.exe\n\n  Java:                          \n    ant:                         NO\n    JNI:                         NO\n    Java wrappers:               NO\n    Java tests:                  NO\n\n  Install to:                    D:/build/opencv/4_7_0/cuda_12_0/install\n-----------------------------------------------------------------\n\n\n\n\nThat was easy but what can I do if I’m not as good at guessing what’s missing. Next I will use the same example again to demonstrate how to find out which DLL’s python is searching for."
  },
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#fix-when-path-to-missing-dlls-is-not-known",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#fix-when-path-to-missing-dlls-is-not-known",
    "title": "“ImportError: DLL load failed while importing cv2: The specified module could not be found.”",
    "section": "Fix when path to missing dll’s is not known",
    "text": "Fix when path to missing dll’s is not known\nTo find which DLL’s are missing we can use process monitor which will enable us to see the names of the DLL’s which python is trying to load.\nProcess monitor by default has produces a significant amount of output so it is a good idea to filter out as much of this noise as you can. To do this you can either load this filter (as suggested by LaurentBerger) with “File-&gt;Import Configuration”, or manually add the filter entries on the “Process Monitor Filter” window, which is opened by default every time you launch the application, details below.\nManually Adding Filter Entries\nSince we only want to view shared libaries which are accessed by the python.exe process, we can add the following filters to make our lives easier 1. Process Name -&gt; is -&gt; python.exe 2. Operation -&gt; is -&gt; CreateFile 3. Result -&gt; is -&gt; NAME NOT FOUND 4. Result -&gt; is -&gt; SUCCESS 5. Path -&gt; contains -&gt; .dll 6. Path -&gt; contains -&gt; .pyd (not striclty necessary, if this was missing the error would be “ModuleNotFoundError: No module named ‘cv2’” but its a shared library so why not)\nYour filter should now resemble the below.\n\nBefore continuing it is advisable to close any other python proceses as the output from these will pollute the main window.\nNow start python and before typing import cv2, press the clear button (red trash can) in process monitor to clear any output generated during python’s initialization.\n\nimport cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\ntitle\n\n\nBecause I have reset the python DLL search path on running import cv2 I get the above output in process monitor which shows that we successfully found cv2.cp3x-win_amd64.pyd (otherwise we would see the “ModuleNotFoundError: No module named ‘cv2’” error) however it also shows several attempts have been made to locate opencv_img_hash_470.dll and opencv_world470.dll without success.\nAs before we add the directory containing these to the python DLL search path.\n\nimport os\nos.add_dll_directory(\"D:\\\\build\\\\opencv\\\\4_7_0\\\\cuda_12_D\\\\bin\")\nimport cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\ntitle\n\n\nNow process monitor shows that opencv_img_hash_470.dll and opencv_world470.dll were located successfully after a few attempts however we are missing nppc64_12.dll which is part of the CUDA SDK. As before if we add the CUDA SDK binary directory to the python DLL search path the call to import cv2 will be successful. If however we were still seeing the same error we could simply repeat the process, that is examine the output in process monitor and add the directories containing the missing DLL’s to the python DLL search path."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html",
    "href": "nbs/opencv4-cuda-streams.html",
    "title": "Overview",
    "section": "",
    "text": "Since Aug 2018 the OpenCV CUDA API has been exposed to python (for details of the API call’s see test_cuda.py). To get the most from this new functionality you need to have a basic understanding of CUDA (most importantly that it is data not task parallel) and its interaction with OpenCV. Below I have tried to introduce these topics with an example of how you could optimize a toy video processing pipeline. The actual functions called in the pipeline are not important, they are simply there to simulate a common processing pipeline consisting of work performed on both the host (CPU) and device (GPU).\nThis guide is taken from a Jupyter Notebook which can be cloned from here. The procedure is as follows, following some quick initialization, we start with a naive implementation on both the CPU and GPU to get a baseline result. We then proceed to incrementally improve the implementation by using the information provided by the Nvidia Visual Profiler.\nOn a laptop GTX2080 paired with an i7-8700 the final CUDA incarnation resulted in a speed up of ~30x and ~10x over the naive CPU and GPU implementations.\nContents: 1. Naive implementations - CPU - GPU - Analysis 2. Pre-allocation of return arrays - CPU - GPU - Analysis 3. CUDA Streams - Replacing the default stream - Analysis - Overlap host and device computation - attempt 1 - Analysis - Overlap host and device computation - attempt 2 - Analysis - Overlap host and device computation - attempt 3 - Analysis - Overlap host and device computation - multiple streams - Analysis 4. Timing without the profiler - Analysis 5. Summary 6. Run outside the notebook\n\nInit\n\n#export\nimport os\nimport time\nimport numpy as np\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n\ndef CheckFg(fg_gs,fg):\n    if (len(fg_gs) != len(fg)):\n        print(f'len(fg_gs) == {len(fg_gs)} and len(fg) == {len(fg)}')\n        return False\n    for i in range(0,len(fg)):\n        if(np.sum(fg_gs[i]!=fg[i]) != 0):\n            print(f'fg_gs[{i}] != fg[{i}]')\n            return i\n    print('Test passed!')\n    return True\n\n\n#export\n# globals\nvidPath = os.environ['OPENCV_TEST_DATA_PATH'] + '/cv/video/768x576.avi'\nlr = 0.05\nrows_big = 1440\ncols_big = 2560\ncheck_res = False\nframe_device = cv.cuda_GpuMat()\n\n\n\n\nNaive implementations\n\n#export\ndef ProcVid0(proc_frame_func,lr):\n    cap = cv.VideoCapture(vidPath)\n    if (cap.isOpened()== False): \n        print(\"Error opening video stream or file\")\n        return\n    n_frames = 0\n    start = time.time()\n    while(cap.isOpened()):\n        ret, frame = cap.read()\n        if ret == True:\n            n_frames += 1 \n            proc_frame_func(frame,lr)\n        else:\n            break\n    end = time.time()\n    cap.release()\n    return (end - start)*1000/n_frames, n_frames;\n\n\n\nCPU\n\n#export\nbgmog2 = cv.createBackgroundSubtractorMOG2()\ndef ProcFrameCPU0(frame,lr,store_res=False):\n    frame_big = cv.resize(frame,(cols_big,rows_big))\n    fg_big = bgmog2.apply(frame_big,learningRate = lr)\n    fg_small = cv.resize(fg_big,(frame.shape[1],frame.shape[0]))\n    if(store_res):\n        cpu_res.append(np.copy(fg_small))\n\n\n#export\ncpu_res = []\ncpu_time_0, n_frames = ProcVid0(partial(ProcFrameCPU0,store_res=check_res),lr)\nprint(f'CPU 0 (naive): {n_frames} frames, {cpu_time_0:.2f} ms/frame')\n\nCPU 0 (naive): 100 frames, 30.05 ms/frame\n\n\n\n\n\nGPU\n\n#export\nbgmog2_device = cv.cuda.createBackgroundSubtractorMOG2()\ndef ProcFrameCuda0(frame,lr,store_res=False):\n    frame_device.upload(frame)\n    frame_device_big = cv.cuda.resize(frame_device,(cols_big,rows_big))\n    fg_device_big = bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null())\n    fg_device = cv.cuda.resize(fg_device_big,frame_device.size())\n    fg_host = fg_device.download()\n    if(store_res):\n        gpu_res.append(np.copy(fg_host))\n\n\n#export\ngpu_res = []\ngpu_time_0, n_frames = ProcVid0(partial(ProcFrameCuda0,store_res=check_res),lr)\nprint(f'GPU 0 (naive): {n_frames} frames, {gpu_time_0:.2f} ms/frame')\nprint(f'Speedup over CPU: {cpu_time_0/gpu_time_0:.2f}')\n\nGPU 0 (naive): 100 frames, 3.92 ms/frame\nSpeedup over CPU: 7.67\n\n\n\n\nAnalysis\n\n\n\ngpu_naive\n\n\nObservations: The output gpu_time_0 from above is the average amount of time to process each frame, recorded on the host. This will be referred to as the frame time and is the value that we want to reduce. In order to achieve this we need to investigate what is actually occurring on the host and device for each frame. Luckily the Nvidia provides a useful visual tool for this, the Nvidia Visual Profiler.\nThe image above shows the Nvidia Visual Profiler output from processing 2 of the 100 frames. Important things to be aware of here are:\n\nThe runtime API calls in brown which in this example represent the time the host (CPU) spends waiting for the device (GPU) calls to return.\nThe remaining blocks which show the time spent on the device. This is split according to the operation (kernel, memset, MemCpy(HtoD), MemCpy(DtoH)) as well as by the CUDA stream which the operations are issued to. In this case everything is issued to the Default stream.\nThe 0.93ms gap in between the blocks of runtime API calls represents the time spent executing code on the host, here that is the time taken for OpenCV to read and decode each video frame, frame = cap.read().\nIn this naive implementation all device calls from the host are synchronous and as a result the difference between (1) and (2) can be interpreted as periods where no useful work is being performed on either the host or the device. The host is blocking waiting for the device to return and the device is also idle, allocating or freeing memory.\n\nFrom now on for convenience, for a single frame, I will refer to 1), 2) and 3) as the runtime API time, device time, host time respectively. As shown the profiler output, the current runtime API time and host time are ~2.38ms and ~0.93ms.\nTaking (1) and (4) into account from left to right the output from the profiler can be mapped to the python calls as:\n\n(1217.62ms-1220ms) proc_frame_func(frame,lr): calls to the device to process the first frame (~2.38ms)\n(1220ms-1220.93ms) frame = cap.read(): read and decode the second video frame on the host (~0.93ms)\n(1220.93ms-) proc_frame_func(frame,lt): calls to the device to process the second frame\n\nClearly from the gaps described in (4) a lot of time is wasted waiting for the device calls to return, and as the host time does not overlap the device time, there is a lot of room for improvement.\nHypothesis: The main causes of (4) are the blocking calls to both - cudaMallocPitch() - OpenCV in python automatically allocates any arrays (NumPy or GpuMat) which are returned from a function call. That is on every iteration &gt; ret, frame = cap.read()causes memory for the NumPy array frame to be allocated and destroyed on the host and &gt; frame_device_big = cv.cuda.resize(frame_device,(cols_big,rows_big)) fg_device_big = bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null()) fg_device = cv.cuda.resize(fg_device_big,frame_device.size())  causes memory for frame_device_big, fg_device_big and fg_device to be allocated and destroyed on the device.\n\ncudaDeviceSynchronise() - if you don’t explicitly pass in a CUDA stream to an OpenCV CUDA function, the default stream will be used and cudaDeviceSynchronize() called before the function exits.\n\nAction: First address the unnecessary calls to cudaMallocPitch(), by pre-allocating any output arrays and passing them as input arguments.\n\n\n\n\n\nPre-allocation of return arrays\n\n#export\ndef ProcVid1(proc_frame,lr):\n    cap = cv.VideoCapture(vidPath)\n    if (cap.isOpened()== False): \n        print(\"Error opening video stream or file\")\n        return\n    n_frames = 0\n    start = time.time()\n    while(cap.isOpened()):\n        ret,_ = cap.read(proc_frame.Frame())\n        if ret == True:\n            n_frames += 1 \n            proc_frame.ProcessFrame(lr)\n        else:\n            break\n    end = time.time()\n    cap.release()\n    return (end - start)*1000/n_frames, n_frames;\n\n\n\nCPU\n\n#export\nclass ProcFrameCpu1:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.createBackgroundSubtractorMOG2()\n        self.frame = np.empty((rows_small,cols_small,3),np.uint8)\n        self.frame_big = np.empty((rows_big,cols_big,3),np.uint8)\n        self.fg_big = np.empty((rows_big,cols_big),np.uint8)\n        self.fg_small = np.empty((rows_small,cols_small),np.uint8)\n        \n    def ProcessFrame(self,lr):\n        cv.resize(self.frame,(self.cols_big,self.rows_big),self.frame_big)\n        self.bgmog2.apply(self.frame_big,self.fg_big,learningRate = lr)\n        cv.resize(self.fg_big,(self.cols_small,self.rows_small),self.fg_small)\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_small))\n        \n    def Frame(self):\n        return self.frame\n    \ncap = cv.VideoCapture(vidPath)\nif (cap.isOpened()== False): \n  print(\"Error opening video stream or file\")\nret, frame = cap.read()\ncap.release()\nrows_small,cols_small = frame.shape[:2]\nproc_frame_cpu1 = ProcFrameCpu1(rows_small,cols_small,rows_big,cols_big,check_res)\n\n\n#export\ncpu_time_1, n_frames = ProcVid1(proc_frame_cpu1,lr)\nprint(f'CPU 1 (pre-allocation): {n_frames} frames, {cpu_time_1:.2f} ms/frame')\nprint(f'Speedup over CPU baseline: {cpu_time_0/cpu_time_1:.2f}')\n\nCPU 1 (pre-allocation): 100 frames, 27.76 ms/frame\nSpeedup over CPU baseline: 1.08\n\n\n\nif check_res: CheckFg(cpu_res,proc_frame_cpu1.res)\n\n\n\n\nGPU\n\n#export\nclass ProcFrameCuda1:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.frame = np.empty((rows_small,cols_small,3),np.uint8)\n        self.frame_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3)\n        self.frame_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3)        \n        self.fg_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1)\n        self.fg_device_big.setTo(0)\n        self.fg_device = cv.cuda_GpuMat(np.shape(frame)[0],np.shape(frame)[1],cv.CV_8UC1)\n        self.fg_host = np.empty((rows_small,cols_small),np.uint8)\n        \n    def ProcessFrame(self,lr):\n        self.frame_device.upload(self.frame)\n        cv.cuda.resize(self.frame_device,(cols_big,rows_big),self.frame_device_big)\n        self.bgmog2.apply(self.frame_device_big,lr,cv.cuda.Stream_Null(),self.fg_device_big)\n        cv.cuda.resize(self.fg_device_big,self.fg_device.size(),self.fg_device)\n        self.fg_device.download(self.fg_host)\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_host))\n        \n    def Frame(self):\n        return self.frame\n    \nproc_frame_cuda1 = ProcFrameCuda1(rows_small,cols_small,rows_big,cols_big,check_res)\n\n\n#export\ngpu_time_1, n_frames = ProcVid1(proc_frame_cuda1,lr)\nprint(f'GPU 1 (pre-allocation): {n_frames} frames, {gpu_time_1:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_0/gpu_time_1:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_1:.2f}')\n\nGPU 1 (pre-allocation): 100 frames, 1.99 ms/frame\nIncremental speedup: 1.96\nSpeedup over CPU: 13.91\n\n\n\nif check_res: CheckFg(gpu_res,proc_frame_cuda1.res)\n\n\n\nAnalysis\n\n\n\ntitle\n\n\nObservations: Pre-allocating the arrays has successfully removed the calls to cudaMallocPitch() and significantly (3 frames are now processed instead of 1.5) reduced (4), the time the host spends waiting for the CUDA runtime to return control to it.\nPre-allocation on the host has also reduced the host time from ~0.93ms to ~0.57ms. The host time will now be unaffected by the remaining changes we make and can be observed to be approximately constant after each of the following optimizations.\nWe will now proceed to try and reduce the runtime API time which in this step has already been fallen from ~2.38ms to ~1.15ms.\nHypothesis: As mentioned above by not specifying a stream all calls are placed in the “Default” stream which can be seen at the bottom of the figure. This means that following each asynchronous kernel launch there will be a synchronizing call to cudaDeviceSynchronize() shown below:\n\ncv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big) async kernel 1, cudaDeviceSynchronize() bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null(),fg_device_big) async kernel 2, cudaDeviceSynchronize() cv.cuda.resize(fg_device_big,fg_device.size(),fg_device) async kernel 3, cudaDeviceSynchronize() fg_device.download(fg_host) synchronous copy from device to host\n\nAction: Pass a non default CUDA stream to each OpenCV CUDA function.\n\n\n\n\n\nCUDA Streams\n\n\nReplacing the default stream\n\n#export\nclass ProcFrameCuda2:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.stream = cv.cuda_Stream()\n        self.frame = np.empty((rows_small,cols_small,3),np.uint8)\n        self.frame_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3)\n        self.frame_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3)\n        self.fg_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1)\n        self.fg_device = cv.cuda_GpuMat(np.shape(frame)[0],np.shape(frame)[1],cv.CV_8UC1)\n        self.fg_host = np.empty((rows_small,cols_small),np.uint8)\n        \n    def ProcessFrame(self,lr):\n        self.frame_device.upload(self.frame,self.stream)\n        cv.cuda.resize(self.frame_device,(cols_big,rows_big),self.frame_device_big,stream=self.stream)\n        self.bgmog2.apply(self.frame_device_big,lr,self.stream,self.fg_device_big)\n        cv.cuda.resize(self.fg_device_big,self.fg_device.size(),self.fg_device,stream=self.stream)\n        self.fg_device.download(self.stream,self.fg_host)\n        self.stream.waitForCompletion()  # imidiate wait\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_host))\n        \n    def Frame(self):\n        return self.frame\n    \nproc_frame_cuda2 = ProcFrameCuda2(rows_small,cols_small,rows_big,cols_big,check_res)\n\n\n#export\ngpu_time_2, n_frames = ProcVid1(proc_frame_cuda2,lr)\nprint(f'GPU 2 (replacing the default stream): {n_frames} frames, {gpu_time_2:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_1/gpu_time_2:.2f}')\nprint(f'Speedup over GPU baseline: {gpu_time_0/gpu_time_2:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_2:.2f}')\n\nGPU 2 (replacing the default stream): 100 frames, 1.90 ms/frame\nIncremental speedup: 1.05\nSpeedup over GPU baseline: 2.07\nSpeedup over CPU: 14.64\n\n\n\nif check_res: CheckFg(gpu_res,proc_frame_cuda2.res)\n\n\n\nAnalysis\n\n\n\ntitle\n\n\nObservations: The calls to cudaDeviceSyncronize() have now been removed, and as a result the gaps between the device calls have disappeared, further reducing the runtime API time from ~1.15ms to ~1.07ms. That said it looks like the calls to cudaDeviceSyncronize() have just been replaced by calls to cudaMemcpy2DAsync().\nHypothesis: What has actually happened is we have tried to use asynchronous copies to and from the device without first pinning the host memory. Therefore what is shown are three asynchronous kernel launches and a synchronous copy from the device to the host, which blocks the host thread until all the previous work on the device is complete: &gt; cv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big,stream=stream) async kernel 1 bgmog2.apply(frame_device_big,lr,stream,fg_device_big) acync kernel 2 cv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3 fg_device.download(stream,fg_host) synchronous copy\nAction: Pin the host memory to address this issue.\n\n\n\n\nOverlap host and device computation - attempt 1\n\n#export\n# host mem not implemented, manually pin memory\nclass PinnedMem(object):\n    def __init__(self, size, dtype=np.uint8):\n        self.array = np.empty(size,dtype)\n        cv.cuda.registerPageLocked(self.array)\n        self.pinned = True\n    def __del__(self):\n        cv.cuda.unregisterPageLocked(self.array)\n        self.pinned = False\n    def __repr__(self):\n        return f'pinned = {self.pinned}'\n\n\n#export\nclass ProcFrameCuda3:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.stream = cv.cuda_Stream()\n        self.frame = PinnedMem((rows_small,cols_small,3))\n        self.frame_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3)\n        self.frame_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3)\n        self.fg_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1)\n        self.fg_device = cv.cuda_GpuMat(np.shape(frame)[0],np.shape(frame)[1],cv.CV_8UC1)\n        self.fg_host = PinnedMem((rows_small,cols_small))\n        \n    def ProcessFrame(self,lr):\n        self.frame_device.upload(self.frame.array,self.stream)\n        cv.cuda.resize(self.frame_device,(cols_big,rows_big),self.frame_device_big,stream=self.stream)\n        self.bgmog2.apply(self.frame_device_big,lr,self.stream,self.fg_device_big)\n        cv.cuda.resize(self.fg_device_big,self.fg_device.size(),self.fg_device,stream=self.stream)\n        self.fg_device.download(self.stream,self.fg_host.array)\n        self.stream.waitForCompletion() # imidiate wait\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_host.array))\n        \n    def Frame(self):\n        return self.frame.array\n    \nproc_frame_cuda3 = ProcFrameCuda3(rows_small,cols_small,rows_big,cols_big,check_res)\n\n\n#export\ngpu_time_3, n_frames = ProcVid1(proc_frame_cuda3,lr)\nprint(f'GPU 3 (overlap host and device - attempt 1): {n_frames} frames, {gpu_time_3:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_2/gpu_time_3:.2f}')\nprint(f'Speedup over GPU baseline: {gpu_time_0/gpu_time_3:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_3:.2f}')\n\nGPU 3 (overlap host and device - attempt 1): 100 frames, 1.83 ms/frame\nIncremental speedup: 1.04\nSpeedup over GPU baseline: 2.14\nSpeedup over CPU: 15.15\n\n\n\nif check_res: CheckFg(gpu_res,proc_frame_cuda3.res)\n\n\n\nAnalysis\n\n\n\ntitle\n\n\nObservations: The output is now more intuitive, that said all that we have done is replace the calls to cudaDeviceSyncronize() with calls to cudaStreamSyncronize().\nHypothesis: We are issuing asynchronous calls to the device and then immediately waiting on the host for them to complete.\n\ncv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big,stream=stream) async kernel 1 bgmog2.apply(frame_device_big,lr,stream,fg_device_big) async kernel 2 cv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3 fg_device.download(stream,fg_host.array) async copy DtoH stream.waitForCompletion() block until kernel 1-3 and copy have finished\n\nWhat we really want to do is overlap host and device computation by issuing asynchronous calls to the device and then performing processing on the host, before waiting for the asynchronous device calls to return. For two frames this would be:\n\nframe_device.upload(frame[0].array,stream) async copy HtoD, frame 0 cv.cuda.resize(frame_device,(n_cols_big,n_rows_big),frame_device_big,stream=stream) async kernel 1, frame 0  bgmog2.apply(frame_device_big,lr,stream,fg_device_big) async kernel 2, frame 0 cv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3, frame 0 fg_device.download(stream,fg_host.array) async copy DtoH, frame 0 ret,_ = cap.read(frame[1].array) host read frame 1  stream.waitForCompletion() block until kernel 1-3 and copy have finished for frame 0\n\nNext: Move the position of the synchronization point to after a new frame has been read as described above. To do this We also need to increase the number of host frame containers to two because moving the sync point means frame 0 may still be in the process of being uploaded to the device when we read frame 1. That is, when we call\n\nret,_ = cap.read(frame[1].array) we have not synced, and we have no way to know if the previous call to frame_device.upload(frame[0].array,stream) has finished, hence we need to write to frame[1].array \n\n\n\n\n\nOverlap host and device computation - attempt 2\n\n#export\ndef ProcVid2(proc_frame,lr,simulate=False):\n    cap = cv.VideoCapture(vidPath)\n    if (cap.isOpened()== False): \n        print(\"Error opening video stream or file\")\n        return\n    n_frames = 0\n    start = time.time()    \n    while(cap.isOpened()):\n        ret,_ = cap.read(proc_frame.Frame())\n        if ret == True:\n            n_frames += 1\n            if not simulate:\n                proc_frame.ProcessFrame(lr)\n        else:\n            break\n    proc_frame.Sync()\n    end = time.time()    \n    cap.release()\n    return (end - start)*1000/n_frames, n_frames;\n\n\n#export\nclass ProcFrameCuda4:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.stream = cv.cuda_Stream()\n        self.frame_num = 0\n        self.i_writable_mem = 0\n        self.frames_in = [PinnedMem((rows_small,cols_small,3)),PinnedMem((rows_small,cols_small,3))]\n        self.frame_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3)\n        self.frame_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3)\n        self.fg_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1)\n        self.fg_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC1)\n        self.fg_host = PinnedMem((rows_small,cols_small))\n        \n    def ProcessFrame(self,lr):\n        self.frame_num += 1\n        if(self.frame_num &gt; 1):\n            self.stream.waitForCompletion() # wait after we have read the next frame\n            if(self.store_res):\n                self.res.append(np.copy(self.fg_host.array))\n        self.frame_device.upload(self.frames_in[self.i_writable_mem].array, self.stream)\n        cv.cuda.resize(self.frame_device, (cols_big,rows_big), self.frame_device_big, stream=self.stream)\n        self.bgmog2.apply(self.frame_device_big, lr, self.stream, self.fg_device_big )\n        cv.cuda.resize(self.fg_device_big, self.fg_device.size(), self.fg_device, stream=self.stream)\n        self.fg_device.download(self.stream,self.fg_host.array)\n        \n    def Frame(self):\n        self.i_writable_mem = (self.i_writable_mem + 1) % len(self.frames_in)\n        return self.frames_in[self.i_writable_mem].array\n    \n    def Sync(self):\n        self.stream.waitForCompletion()\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_host.array))\n    \nproc_frame_cuda4 = ProcFrameCuda4(rows_small,cols_small,rows_big,cols_big,check_res)\n\n\n#export\ngpu_time_4, n_frames = ProcVid2(proc_frame_cuda4,lr)\nprint(f'GPU 4 (overlap host and device - attempt 2): {n_frames} frames, {gpu_time_4:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_3/gpu_time_4:.2f}')\nprint(f'Speedup over GPU baseline: {gpu_time_0/gpu_time_4:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_4:.2f}')\n\nGPU 4 (overlap host and device - attempt 2): 100 frames, 1.83 ms/frame\nIncremental speedup: 1.00\nSpeedup over GPU baseline: 2.14\nSpeedup over CPU: 15.18\n\n\n\nif check_res: CheckFg(gpu_res,proc_frame_cuda4.res)\n\n\n\nAnalysis\n\n\n\ntitle\n\n\nObservations: At first glance changing the synchronization point does not appear to have done anything the cudaStreamSynchronize() (stream.waitForCompletion()) still starts at the point just before the frame is processed on the device. On closer inspection we can see that the runtime API time (~1.5ms) now begins much earlier than the device time (~0.8ms) and as we intended overlaps the host time. That said we are not seeing any host/device processing overlap, so whats going on?\nHypothesis: This is most likely to be because we are working on Windows where the GPU is a Windows Display Driver Model device. See below for more details.\n\nCUDA driver has a software queue for WDDM devices to reduce the average overhead of submitting command buffers to the WDDM KMD driver\n\nThis would cause all the device calls from the previous frame to be queued and then issued when we call stream.waitForCompletion() and could explain the profiler output.\nNext: Test the hypothesis by forcing the CUDA driver to dispatch all queued calls by issuing a call to stream.queryIfComplete() as shown below.\n\nframe_device.upload(frames_in[0].array, stream) async copy HtoD, frame 0 cv.cuda.resize(frame_device,(n_cols_big,n_rows_big),frame_device_big,stream=stream) async kernel 1, frame 0  bgmog2.apply(frame_device_big, lr, stream, fg_device_big ) async kernel 2, frame 0 cv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3, frame 0 fg_device.download(stream,fg_host.array) async copy DtoH, frame 0 stream.queryIfComplete() force WDDM to dispatch any qued device calls ret,_ = cap.read(frame[1].array) host read frame 1  stream.waitForCompletion() block until kernel 1-3 and copy have finished for frame 0\n\n\n\n\n\nOverlap host and device computation - attempt 3\n\n#export\nclass ProcFrameCuda5:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.stream = cv.cuda_Stream()\n        self.frame_num = 0\n        self.i_writable_mem = 0\n        self.frames_in = [PinnedMem((rows_small,cols_small,3)),PinnedMem((rows_small,cols_small,3))]\n        self.frame_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3)\n        self.frame_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3)\n        self.fg_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1)\n        self.fg_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC1)\n        self.fg_host = PinnedMem((rows_small,cols_small))\n        \n    def ProcessFrame(self,lr):\n        self.frame_num += 1\n        if(self.frame_num &gt; 1):\n            self.stream.waitForCompletion() # wait after we have read the next frame\n            if(self.store_res):\n                self.res.append(np.copy(self.fg_host.array))\n        self.frame_device.upload(self.frames_in[self.i_writable_mem].array, self.stream)\n        cv.cuda.resize(self.frame_device, (cols_big,rows_big), self.frame_device_big, stream=self.stream)\n        self.bgmog2.apply(self.frame_device_big, lr, self.stream, self.fg_device_big )\n        cv.cuda.resize(self.fg_device_big, self.fg_device.size(), self.fg_device, stream=self.stream)\n        self.fg_device.download(self.stream,self.fg_host.array)\n        self.stream.queryIfComplete() # kick WDDM\n        \n    def Frame(self):\n        self.i_writable_mem = (self.i_writable_mem + 1) % len(self.frames_in)\n        return self.frames_in[self.i_writable_mem].array\n    \n    def Sync(self):\n        self.stream.waitForCompletion()\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_host.array))\n    \nproc_frame_cuda5 = ProcFrameCuda5(rows_small,cols_small,rows_big,cols_big,check_res)\n\n\n#export\ngpu_time_5, n_frames = ProcVid2(proc_frame_cuda5,lr)\nprint(f'GPU 5 (overlap host and device - attempt 3): {n_frames} frames, {gpu_time_5:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_4/gpu_time_5:.2f}')\nprint(f'Speedup over GPU baseline: {gpu_time_0/gpu_time_5:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_5:.2f}')\n\nGPU 5 (overlap host and device - attempt 3): 100 frames, 1.23 ms/frame\nIncremental speedup: 1.49\nSpeedup over GPU baseline: 3.19\nSpeedup over CPU: 22.60\n\n\n\nif check_res:  CheckFg(gpu_res,proc_frame_cuda5.res)\n\n\n\nAnalysis\n\n\n\ntitle\n\n\nObservations: It appears as though the WDDM driver was at fault, by including the extra call to stream.queryIfComplete() we have finally overlapped the processing on the host and device. This can be observed in the profiler output where the host time (~0.62ms), overlaps the device time (~0.79ms) in Stream 2017. Notice also that there are gaps between the blocks of device time in Stream 2017 with the runtime API time (~1.07ms) still starting sometime before the device time and ending exactly after the Memcpy (DtoH) (fg_device.download(stream,fg_host.array)).\nMost importantly the device is almost saturated with only the small gap (~0.2ms) in between each block representing the device time for each frame in Stream 2017. So what is causing this small gap?\nHypothesis: The device is stalling.\nAs already mentioned the host time cannot be changed. Additionally from the profiler output it is clear that the host time (~0.62ms) is less than the device time (~0.79ms).\nThat is given the processing pipeline below &gt; Process frame 0 on the device ~0.79ms (copy frame 0 to the device execute kernel 1-3 and copy back to the host) ret,_ = cap.read(frame[1].array) ~0.62ms (read frame 1 on the host)  stream.waitForCompletion() block for (~0.17ms = 0.79ms-0.62ms) until processing for frame 0 has finished\nstream.waitForCompletion() will on average cause the host to wait ~0.17ms for the device processing to finish. This can be observed in the profiler output by the length of cudaStreamchronize() which for each frame ends exactly following the Memcpy(DtoH). Unfortunately this wait stalls the device because it has no work to perform until more calls are issued by the host, which in this case does not occur until after the call to stream.waitForCompletion(). If only there was a way to issue work to the device in advance of stream.waitForCompletion(), which will continue to be performed afterwards.\nFortunately there is by using multiple streams, each processing a single frame at a time. This allows us to issue commands in advance, to process frame 1 before we start the wait on the host for frame 0, shown below\n\nret,_ = cap.read(frame[0].array)host read frame 0  i)Process frame 0 in stream 0 ret,_ = cap.read(frame[1].array)host read frame 1  ii)Process frame 1 in stream 1 ret,_ = cap.read(frame[2].array)host read frame 2  stream[0].waitForCompletion() block until i) the processing for frame 0 has finished, allowing the device to continue with ii)  iii)Process frame 2 in stream 0 ret,_ = cap.read(frame[0].array)host read frame 3  stream[1].waitForCompletion() block until ii) the processing for frame 1 has finished, allowing the device to continue with iii) …\n\nNotice that when stream[0].waitForCompletion() is called the device has Process frame 1 in stream 1 already queued up in stream 1 meaning that the wait on the host should not cause a stall on the device.\nNote: Using multiple streams in this way will add additional latency and is not going to be suitable for real time processing, that said the additional latency in most real world cases will be tolerable and worth the reduction in processing time.\nNext: Use multiple streams.\n\n\n\n\nOverlap host and device computation - multiple streams\n\n#export\nclass SyncType():\n    none = 1\n    soft = 2\n    hard = 3\n    \nclass ProcFrameCuda6:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,n_streams,store_res=False,sync=SyncType.soft,device_timer=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.n_streams = n_streams\n        self.store_res = store_res        \n        self.sync = sync\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.frames_device = []\n        self.frames_device_big = []\n        self.fgs_device_big = []\n        self.fgs_device = []\n        self.fgs_small = []   \n        self.streams = []\n        self.frames = []\n        self.InitMem()\n        self.InitStreams()\n        self.res = []\n        self.i_stream = 0        \n        self.n_frames = 0\n        self.i_writable_mem = 0\n        self.device_timer = device_timer\n        if self.device_timer:\n            self.events_start = []\n            self.events_stop = []\n            self.InitEvents()\n            self.device_time = 0\n        \n    def InitMem(self):\n        for i in range(0,self.n_streams + 1):\n            self.frames.append(PinnedMem((rows_small,cols_small,3)))\n            \n        for i in range(0,self.n_streams):\n            self.frames_device.append(cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3))\n            self.frames_device_big.append(cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3))\n            self.fgs_device_big.append(cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1))\n            self.fgs_device.append(cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC1))\n            self.fgs_small.append(PinnedMem((rows_small,cols_small)))\n                    \n    def InitStreams(self):\n        for i in range(0,self.n_streams): \n            if self.sync == SyncType.hard:\n                self.streams.append(cv.cuda.Stream_Null())\n            elif self.sync == SyncType.soft:\n                self.streams.append(cv.cuda_Stream())\n                \n    def InitEvents(self):\n        for i in range(0,self.n_streams):\n            self.events_start.append(cv.cuda_Event())\n            self.events_stop.append(cv.cuda_Event()) \n            \n    def IncStream(self):\n        self.i_stream = (self.i_stream+1)%self.n_streams\n        \n    def ProcessFrame(self,lr):\n        self.n_frames += 1\n        i = self.i_stream\n        self.IncStream()\n        stream = self.streams[i]\n        if(self.n_frames &gt; self.n_streams and self.sync != SyncType.none):            \n            stream.waitForCompletion() # wait once both streams are used               \n            if self.device_timer:  self.device_time += cv.cuda_Event.elapsedTime(self.events_start[i],self.events_stop[i])\n            if(self.store_res):\n                self.res.append(np.copy(self.fgs_small[i].array))\n        if self.device_timer: self.events_start[i].record(stream)\n        self.frames_device[i].upload(self.frames[self.i_writable_mem].array,stream)\n        cv.cuda.resize(self.frames_device[i], (cols_big,rows_big), self.frames_device_big[i], stream=stream)\n        self.bgmog2.apply(self.frames_device_big[i], lr, stream, self.fgs_device_big[i])\n        cv.cuda.resize(self.fgs_device_big[i], self.fgs_device[i].size(), self.fgs_device[i], stream=stream)\n        self.fgs_device[i].download(stream, self.fgs_small[i].array)\n        if self.device_timer: self.events_stop[i].record(stream)\n        stream.queryIfComplete() # kick WDDM       \n        \n    def Frame(self):\n        self.i_writable_mem = (self.i_writable_mem + 1) % len(self.frames)\n        return self.frames[self.i_writable_mem].array\n    \n    def Sync(self):\n        # sync on last frames\n        if (self.sync == SyncType.none):\n            return\n        \n        for i in range(0,self.n_streams):\n            if(not self.streams[self.i_stream].queryIfComplete()):\n                self.streams[self.i_stream].waitForCompletion()\n            if(self.store_res):\n                self.res.append(np.copy(self.fgs_small[self.i_stream].array))\n            self.IncStream()        \n            \n    def FrameTimeMs(self):\n        if self.device_timer:\n            return self.device_time/self.n_frames\n        else:\n            return 0\n            \nproc_frame_cuda6 = ProcFrameCuda6(rows_small,cols_small,rows_big,cols_big,2,check_res,SyncType.soft)\n\n\n#export\ngpu_time_6, n_frames = ProcVid2(proc_frame_cuda6,lr)\nprint(f'GPU 6 (multiple streams): {n_frames} frames, {gpu_time_6:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_5/gpu_time_6:.2f}')\nprint(f'Speedup over GPU baseline: {gpu_time_0/gpu_time_6:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_6:.2f}')\n\nGPU 6 (multiple streams): 100 frames, 0.97 ms/frame\nIncremental speedup: 1.27\nSpeedup over GPU baseline: 4.05\nSpeedup over CPU: 28.68\n\n\n\nif check_res: CheckFg(gpu_res,proc_frame_cuda6.res)\n\n\n\nAnalysis\n\n\n\ntitle\n\n\nObservations: The device is now completely saturated with memory operations overlapping kernel executions in Streams 2418 and 2419. Additionally the host and device time completely overlap each other. By saturating the device and overlapping host and device computation we have probably reached the limit of the optimizations we can apply to this particular toy problem.\nNotice that as a result of the kernel/memory overlap the average device time is no longer equal to the average amount of time to process a frame on the device (streamed device time). In fact because of kernel/memory and host/device overlap the average device time should now be greater than both the average streamed device and frame time.\nHypothesis: If the above assumption is correct we should be able to see this effect by using device timers to get a more accurate value for the average device time.\nNext: 1. Use device timers to get the average device time. Unfortunately this introduces some overhead so we will have to compare this to the average time required to process each frame calculated without the device timers. This may mean that we may not see the difference that we expect. 2. Calculate the theoretical average time to process each frame on the host and then the device without overlap (host time + device time), to see the gain from host/device and kernel/memory overlap. 3. Calculate the average wasted time on the host (streamed device time - host time) time where the host could be performing useful operations without increasing the average processing time).\n\n\n\n\n\nTiming without the profiler\n\n#export\nproc_frame_cuda7 = ProcFrameCuda6(rows_small,cols_small,rows_big,cols_big,2,check_res,SyncType.soft,True)\nProcVid2(proc_frame_cuda7,lr)\nprint(f'Mean times calculated over {n_frames} frames:')\nprint(f'Time to process each frame on the device: {proc_frame_cuda7.FrameTimeMs():.2f} ms/frame')\nprint(f'Time to process each frame (host/device): {gpu_time_6:.2f} ms/frame')\nprint(f'-&gt; Gain from memcpy/kernel overlap if device is saturated: {proc_frame_cuda7.FrameTimeMs()-gpu_time_6:.2f} ms/frame')\nhostTime, n_frames = ProcVid2(proc_frame_cuda6, lr, True)\nprint(f'Time to read and decode each frame on the host: {hostTime:.2f} ms/frame')\nprint(f'-&gt; Total processing time host + device: {proc_frame_cuda7.FrameTimeMs()+hostTime:.2f} ms/frame')\nprint(f'-&gt; Gain from host/device overlap: {proc_frame_cuda7.FrameTimeMs()+hostTime - gpu_time_6:.2f} ms/frame')\nprint(f'-&gt; Currently waisted time on host: {gpu_time_6-hostTime:.2f} ms/frame')\n\nMean times calculated over 100 frames:\nTime to process each frame on the device: 1.00 ms/frame\nTime to process each frame (host/device): 0.97 ms/frame\n-&gt; Gain from memcpy/kernel overlap if device is saturated: 0.04 ms/frame\nTime to read and decode each frame on the host: 0.71 ms/frame\n-&gt; Total processing time host + device: 1.71 ms/frame\n-&gt; Gain from host/device overlap: 0.74 ms/frame\n-&gt; Currently waisted time on host: 0.26 ms/frame\n\n\n\n\nAnalysis\nObservations: 1. It appears that we gained 0.04 ms/frame from the kernel/memory processing overlap on the device. Unfortunately we cannot say this for sure because the times compared here are from two separate runs due to the device timer overhead. That said, the implication is that our interpretation of the kernel/memory overlap seen in the Nvidia Visual Profiler is correct. 2. The total processing which needs to be performed on the host and device takes and average of 1.71 ms/frame which is 0.74 ms/frame greater than our final implementation, demonstrating the importance of using asynchronous device calls and CUDA streams. 3. We have 0.26 ms/frame to spare on the host which we can make use of without affecting the average frame time of 0.97 ms/frame.\n\n\n\n\nSummary\nWhen calling OpenCV CUDA functions the most effective optimizations (in order of effectiveness/ease to implement) for this toy problem are given below. Whilst (1) will always be effective, the other optimizations will heavily depend on the CPU/GPU specifications, data size and the amount of processing which can be performed on the device before returning to the host. Therefore it is always beneficial to use a tool such as the Nvidia visual profiler to analyze your pipeline as you make changes. 1. Pre-allocate and pass all Numpy and/or GpuMat arrays (making sure they are the correct size) as function arguments to avoid them being allocated each time the function is called. 2. Try to design a processing pipeline which allows memory copies to overlap kernel calls and work to be performed on both the host and the device at the same time. 3. Use CUDA streams with pinned host memory and if you are working on windows consider calling stream.queryIfComplete() to force the WDDM driver to dispatch the CUDA calls. 4. Use multiple streams.\n\n\n\nRun outside the notebook\n\n# taken from https://github.com/fastai/fastai_docs/blob/master/dev_nb/notebook2script.py\n!python notebook2script.py \"opencv4-cuda-streams.ipynb\"\n\nConverted opencv4-cuda-streams.ipynb to exp\\nb_opencv4-cuda-streams.py\n\n\n\n! python exp/nb_opencv4-cuda-streams.py\n\nCPU 0 (naive): 100 frames, 29.74 ms/frame\nGPU 0 (naive): 100 frames, 9.06 ms/frame\nSpeedup over CPU: 3.28\nCPU 1 (pre-allocation): 100 frames, 27.59 ms/frame\nSpeedup over CPU baseline: 1.08\nGPU 1 (pre-allocation): 100 frames, 1.93 ms/frame\nIncremental speedup: 4.69\nSpeedup over CPU: 14.29\nGPU 2 (replacing the default stream): 100 frames, 1.82 ms/frame\nIncremental speedup: 1.06\nSpeedup over GPU baseline: 4.99\nSpeedup over CPU: 15.18\nGPU 3 (overlap host and device - attempt 1): 100 frames, 1.72 ms/frame\nIncremental speedup: 1.06\nSpeedup over GPU baseline: 5.28\nSpeedup over CPU: 16.06\nGPU 4 (overlap host and device - attempt 2): 100 frames, 1.72 ms/frame\nIncremental speedup: 1.00\nSpeedup over GPU baseline: 5.27\nSpeedup over CPU: 16.03\nGPU 5 (overlap host and device - attempt 3): 100 frames, 1.11 ms/frame\nIncremental speedup: 1.55\nSpeedup over GPU baseline: 8.17\nSpeedup over CPU: 24.88\nGPU 6 (multiple streams): 100 frames, 0.90 ms/frame\nIncremental speedup: 1.23\nSpeedup over GPU baseline: 10.03\nSpeedup over CPU: 30.54\nMean times calculated over 100 frames:\nTime to process each frame on the device: 0.93 ms/frame\nTime to process each frame (host/device): 0.90 ms/frame\n-&gt; Gain from memcpy/kernel overlap if device is saturated: 0.03 ms/frame\nTime to read and decode each frame on the host: 0.65 ms/frame\n-&gt; Total processing time host + device: 1.58 ms/frame\n-&gt; Gain from host/device overlap: 0.68 ms/frame\n-&gt; Currently waisted time on host: 0.26 ms/frame\n[ INFO:0] global E:\\Dev\\Repos\\opencv_fork_1\\modules\\videoio\\src\\videoio_registry.cpp (187) cv::`anonymous-namespace'::VideoBackendRegistry::VideoBackendRegistry VIDEOIO: Enabled backends(7, sorted by priority): FFMPEG(1000); GSTREAMER(990); INTEL_MFX(980); MSMF(970); DSHOW(960); CV_IMAGES(950); CV_MJPEG(940)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#building-opencv-with-cuda-using-visual-studio-solution-files-from-the-command-prompt-cmd",
    "href": "accelerate_opencv_cuda_python.html#building-opencv-with-cuda-using-visual-studio-solution-files-from-the-command-prompt-cmd",
    "title": "Accelerate OpenCV on Windows with CUDA and Python",
    "section": "Building OpenCV with CUDA using Visual Studio solution files from the command prompt (cmd)",
    "text": "Building OpenCV with CUDA using Visual Studio solution files from the command prompt (cmd)\nThe following steps will build the opencv_worldxxx.dll using NVIDIA’s recommended settings for future hardware compatibility. This does however have two drawbacks, first the build can take several hours to complete and second, the shared library can be over 1GB depending on the configuration that you choose. To find out how to reduce both the compilation time and size of opencv_worldxxx.dll read choosing a suitable CUDA compute capability first and then continue as below. Additionally to reduce the build time futher you can use the Ninja build system, see building OpenCV with the ninja build system to reduce the build time\n\nOpen windows command prompt, type cmd in the search bar.\nPaste the below into to the command prompt and press Enter.\n\n\n\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -B\"PATH_TO_BUILD_DIR\" -H\"PATH_TO_OPENCV_SOURCE\" -DOPENCV_EXTRA_MODULES_PATH=\"PATH_TO_OPENCV_CONTRIB_MODULES\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DWITH_CUFFT=ON -DCUDA_ARCH_PTX=9.0 -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=PATH_TO_PYTHON_DIST/include -DPYTHON3_LIBRARY=PATH_TO_PYTHON_DIST/libs/python%pyVer%.lib -DPYTHON3_EXECUTABLE=PATH_TO_PYTHON_DIST/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=PATH_TO_PYTHON_DIST/lib/site-packages/numpy/core/include -DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/\nwhere\n\nPATH_TO_OPENCV_SOURCE is the root of the OpenCV files you downloaded or cloned (the directory containing 3rdparty, apps, build, etc.),\nPATH_TO_OPENCV_CONTRIB_MODULES is the path to the modules directory inside the opencv-contrib repository (the directory containing cudaarithm, cudabgsegm, etc.),\nPATH_TO_BUILD_DIR is the path to the directory where the build files should go and\nPATH_TO_PYTHON_DIST is the directory where miniconda was installed and,\nPYTHON_VERSION is the concatination of the major and minor version of your python install, e.g. for Python 3.10.10 PYTHON_VERSION==310.\n\nThis will build OpenCV and its python bindings with CUDA acceleration including all the corresponding tests and examples for verifcation. Additionally if the Nvidia Video Codec SDK or cuDNN are installed the corresponding modules will automatically be included.\nExpand the tip below for an example of the output you should get in CMake if the configuration step is successful.\n\nAn additionaly option you may want to include is -DCUDA_FAST_MATH=ON which compiles the CUDA kernels with the -use_fast_math option. This will however cause some of the accuracy and performace tests to fail as the floating point results will be slightly less accurate.To get the python version open the MiniForge Prompt from the start menu and type Python -VIf you get the following error “CUDA : OpenCV requires enabled ‘cudev’ module from ‘opencv_contrib’” when configuring the build with CMake you have not set OPENCV_EXTRA_MODULES_PATH correctly, most likely you have set it to the root of the opencv_contrib repo and not the modules directory inside the repo.\n\n\n\n\n\nExample of CMake Configuration Output\n\n\n\n\n\n--\n-- General configuration for OpenCV 4.7.0-dev =====================================\n--   Version control:               4.7.0-252-g88a438e542\n--\n--   Extra modules:\n--     Location (extra):            D:/repos/opencv/contrib/modules\n--     Version control (extra):     4.7.0-42-ga42b8bef\n--\n--   Platform:\n--     Timestamp:                   2023-05-03T10:21:52Z\n--     Host:                        Windows 10.0.22621 AMD64\n--     CMake:                       3.25.1\n--     CMake generator:             Visual Studio 17 2022\n--     CMake build tool:            C:/Program Files/Microsoft Visual Studio/2022/Community/MSBuild/Current/Bin/amd64/MSBuild.exe\n--     MSVC:                        1934\n--     Configuration:               Debug Release\n--\n--   CPU/HW features:\n--     Baseline:                    SSE SSE2 SSE3\n--       requested:                 SSE3\n--     Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX\n--       requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n--       SSE4_1 (18 files):         + SSSE3 SSE4_1\n--       SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n--       FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n--       AVX (8 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n--       AVX2 (36 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n--       AVX512_SKX (8 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX\n--\n--   C/C++:\n--     Built as dynamic libs?:      YES\n--     C++ standard:                11\n--     C++ Compiler:                C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.34.31933/bin/Hostx64/x64/cl.exe  (ver 19.34.31937.0)\n--     C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MD /O2 /Ob2 /DNDEBUG\n--     C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MDd /Zi /Ob0 /Od /RTC1\n--     C Compiler:                  C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.34.31933/bin/Hostx64/x64/cl.exe\n--     C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP   /MD /O2 /Ob2 /DNDEBUG\n--     C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP /MDd /Zi /Ob0 /Od /RTC1\n--     Linker flags (Release):      /machine:x64  /INCREMENTAL:NO\n--     Linker flags (Debug):        /machine:x64  /debug /INCREMENTAL\n--     ccache:                      NO\n--     Precompiled headers:         NO\n--     Extra dependencies:          cudart_static.lib nppc.lib nppial.lib nppicc.lib nppicom.lib nppidei.lib nppif.lib nppig.lib nppim.lib nppist.lib nppisu.lib nppitc.lib npps.lib cublas.lib cufft.lib -LIBPATH:C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\n--     3rdparty dependencies:\n--\n--   OpenCV modules:\n--     To be built:                 aruco barcode bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab wechat_qrcode world xfeatures2d ximgproc xobjdetect xphoto\n--     Disabled:                    -\n--     Disabled by dependency:      -\n--     Unavailable:                 alphamat cvv freetype hdf java julia matlab ovis python2 python2 sfm viz\n--     Applications:                tests perf_tests examples apps\n--     Documentation:               NO\n--     Non-free algorithms:         NO\n--\n--   Windows RT support:            NO\n--\n--   GUI:\n--     Win32 UI:                    YES\n--     VTK support:                 NO\n--\n--   Media I/O:\n--     ZLib:                        build (ver 1.2.13)\n--     JPEG:                        build-libjpeg-turbo (ver 2.1.3-62)\n--       SIMD Support Request:      YES\n--       SIMD Support:              NO\n--     WEBP:                        build (ver encoder: 0x020f)\n--     PNG:                         build (ver 1.6.37)\n--     TIFF:                        build (ver 42 - 4.2.0)\n--     JPEG 2000:                   build (ver 2.4.0)\n--     OpenEXR:                     build (ver 2.3.0)\n--     HDR:                         YES\n--     SUNRASTER:                   YES\n--     PXM:                         YES\n--     PFM:                         YES\n--\n--   Video I/O:\n--     DC1394:                      NO\n--     FFMPEG:                      YES (prebuilt binaries)\n--       avcodec:                   YES (58.134.100)\n--       avformat:                  YES (58.76.100)\n--       avutil:                    YES (56.70.100)\n--       swscale:                   YES (5.9.100)\n--       avresample:                YES (4.0.0)\n--     GStreamer:                   NO\n--     DirectShow:                  YES\n--     Media Foundation:            YES\n--       DXVA:                      YES\n--\n--   Parallel framework:            Concurrency\n--\n--   Trace:                         YES (with Intel ITT)\n--\n--   Other third-party libraries:\n--     Intel IPP:                   2021.8 [2021.8.0]\n--            at:                   D:/build/opencv/4_7_0/delete_this/3rdparty/ippicv/ippicv_win/icv\n--     Intel IPP IW:                sources (2021.8.0)\n--               at:                D:/build/opencv/4_7_0/delete_this/3rdparty/ippicv/ippicv_win/iw\n--     Lapack:                      NO\n--     Eigen:                       NO\n--     Custom HAL:                  NO\n--     Protobuf:                    build (3.19.1)\n--     Flatbuffers:                 builtin/3rdparty (23.1.21)\n--\n--   NVIDIA CUDA:                   YES (ver 12.1, CUFFT CUBLAS NVCUVID NVCUVENC)\n--     NVIDIA GPU arch:\n--     NVIDIA PTX archs:            90\n--\n--   cuDNN:                         NO\n--\n--   OpenCL:                        YES (NVD3D11)\n--     Include path:                D:/repos/opencv/opencv/3rdparty/include/opencl/1.2\n--     Link libraries:              Dynamic load\n--\n--   Python 3:\n--     Interpreter:                 C:/Users/username/mambaforge/python.exe (ver 3.9.16)\n--     Libraries:                   C:/Users/username/mambaforge/libs/python39.lib (ver 3.9.16)\n--     numpy:                       C:/Users/username/mambaforge/Lib/site-packages/numpy/core/include (ver 1.23.5)\n--     install path:                C:/Users/username/mambaforge/Lib/site-packages/cv2/python-3.9\n--\n--   Python (for build):            C:/Users/username/mambaforge/python.exe\n--\n--   Java:\n--     ant:                         NO\n--     JNI:                         NO\n--     Java wrappers:               NO\n--     Java tests:                  NO\n--\n--   Install to:                    D:/build/opencv/4_7_0/install\n-- -----------------------------------------------------------------\n--\n-- Configuring done\n-- Generating done\n-- Build files have been written to: D:/build/opencv/4_7_0\n\n\n\n\n\n\n\n\n\nOnly build cuDNN backend\n\n\n\n\n\nIf you just want to CUDA accelerate the DNN module and are not interested in building the rest of the CUDA modules the following can be added to the above command.\n-DBUILD_opencv_cudaarithm=OFF -DBUILD_opencv_cudabgsegm=OFF -DBUILD_opencv_cudafeatures2d=OFF -DBUILD_opencv_cudafilters=OFF -DBUILD_opencv_cudaimgproc=OFF -DBUILD_opencv_cudalegacy=OFF -DBUILD_opencv_cudaobjdetect=OFF -DBUILD_opencv_cudaoptflow=OFF -DBUILD_opencv_cudastereo=OFF -DBUILD_opencv_cudawarping=OFF -DBUILD_opencv_cudacodec=OFF\nThis will significantly reduce compilation time and size of opencv_worldxxx.dll.\n\n\n\n\n\n\n\n\n\nBuild without python bindings\n\n\n\n\n\nIf you don’t want to build python bindings then you can simply remove everything from -DBUILD_opencv_python3 onwards in the above command.\n\n\n\n\n\n\n\n\n\nVerify configuration includes Python bindings before building\n\n\n\n\n\nIf you are building the python bindings look for python3 in the To be built: section of your CMake configuration output and if its not present look for any python related errors in the output preceeding it. e.g.\n--   OpenCV modules:\n--     To be built:                 aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dpm face features2d flann fuzzy hfs highgui img_hash imgcodecs imgproc line_descriptor ml objdetect optflow phase_unwrapping photo plot python2 python3 quality reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab world xfeatures2d ximgproc xobjdetect xphoto\n\n\n\n\nThe OpenCV.sln solution file should now be in your PATH_TO_BUILD_DIR directory. To build OpenCV you have two options depending on you preference you can:\n\nBuild directly from the command line by simply entering the following (swaping Release for Debug to build a release version)\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target INSTALL --config Debug\nBuild through Visual Studio GUI by opening up the OpenCV.sln in Visual Studio, selecting your Configuration, clicking on Solution Explorer, expanding MakeTargets, right clicking on INSTALL and clicking Build.\n\n\nEither approach will both build the library, install the Python bindings and copy the necessary redistributable parts to the install directory (PATH_TO_BUILD_DIR/build/install in this example). All that is required now to run any programs compiled against these libs is to add the directory containing opencv_worldxxx.dll to you user path environmental variable.\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default you have to build Release when generating python bindings, see for instructions on how to build Debug see generate python bindings for a debug build\n\n\nIf everything was successful, congratulations, you now have OpenCV built with CUDA. To quickly verify that the CUDA modules are working and check if there is any performance benefit on your specific hardware see below"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#decreasing-the-build-time-with-ninja",
    "href": "accelerate_opencv_cuda_python.html#decreasing-the-build-time-with-ninja",
    "title": "Accelerate OpenCV on Windows with CUDA and Python",
    "section": "Decreasing the build time with Ninja",
    "text": "Decreasing the build time with Ninja\nThe build time for OpenCV can be reduced by more than 2x (from 2 hours to 30 mins on an i7-8700) by utilizing the Ninja build system instead of directly generating Visual Studio solution files. The only difference you may notice is that Ninja will only produce one configuration at a time, either a Debug or Release, therefore if you don’t want to build Release (the default) the CMAKE_BUILD_TYPE has to be passed to CMake.\nNinja is installed by default if the Desktop development with C++ workload is selected when installing Visual Studio, therefore building with Ninja only requires two extra configuration steps, expand the tip below for an example of the modified command line arguments.:\n\nConfiguring Visual Studio Development tools by entering the following into the command prompt before entering the CMake command (changing Community to either Professional or Enterprise if necessary)\n\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\nTelling CMake to use Ninja instead of Visual Studio, i.e. replacing -G\"Visual Studio 17 2022\" with -GNinja.\n\nOnce the build files have been generated the build can be kicked off in the same way as before but this time dropping the redundant --config argument.\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target INSTALL\n\n\n\n\n\n\nTip\n\n\n\n\n\nExample of the full command line for building a Release version of OpenCV with the Ninja build system.\n\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -B\"PATH_TO_BUILD_DIR\" -H\"PATH_TO_OPENCV_SOURCE\" -DOPENCV_EXTRA_MODULES_PATH=\"PATH_TO_OPENCV_CONTRIB_MODULES\" -G\"Ninja\" -DCMAKE_BUILD_TYPE=Release -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DCUDA_ARCH_PTX=8.6 -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=PATH_TO_PYTHON_DIST include -DPYTHON3_LIBRARY=PATH_TO_PYTHON_DIST/libs/python%pyVer%.lib -DPYTHON3_EXECUTABLE=PATH_TO_PYTHON_DIST/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=PATH_TO_PYTHON_DIST/lib/site-packages/numpy/core/include -DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/ -DOPENCV_SKIP_PYTHON_LOADER=ON\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target INSTALL"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#generate-python-bindings-for-a-debug-build",
    "href": "accelerate_opencv_cuda_python.html#generate-python-bindings-for-a-debug-build",
    "title": "Accelerate OpenCV on Windows with CUDA and Python",
    "section": "Generate Python bindings for a Debug Build",
    "text": "Generate Python bindings for a Debug Build\nPython bindings cannot by default be generated for a Debug configuration, that is unless you have specificaly built or downloaded a debug version of python. That said you can easily generate a Debug build by modifying the contents of PATH_TO_PYTHON_DIST/include/pyconfig.h, changing\npragma comment(lib,\"pythonxx_d.lib\")\nto\npragma comment(lib,\"pythonxx.lib\")\nand\n#       define Py_DEBUG\nto\n//#       define Py_DEBUG"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#troubleshooting-python-bindings-installation-issues",
    "href": "accelerate_opencv_cuda_python.html#troubleshooting-python-bindings-installation-issues",
    "title": "Accelerate OpenCV on Windows with CUDA and Python",
    "section": "Troubleshooting Python Bindings Installation issues",
    "text": "Troubleshooting Python Bindings Installation issues\nIf you are unable to import cv2 without errors then check below to see\n\n\nModuleNotFoundError: No module named ‘cv2’\n\nThe installation of the python bindings has failed, check\n\nthe build was successful, and\n-DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/ was set correctly, and\nif you are still seeing the above error try installing the bindings manually following\n\n\nImportError: ERROR: recursion is detected during loading of “cv2” binary extensions. Check OpenCV installation.\n\nThe main two reasons for this are:\n\nYou have another installation of OpenCV, either manually installed or through the package manager pip/mamba. This can easily be fixed by first uninstalling any opencv-python distributions from your package manager and then deleting the … directory or …. if they existing\nYou have built a Debug configuration. Currently (see issue) when building this configuration the .pyd is not copied into. This can be resolved by creating a .. and copyting the pyd.\n\n\nImportError: DLL load failed: The specified procedure could not be found.\n\nThe directory of one or more of the required DLL’s has not been added with os.add_dll_directory. Whilst the automatic installation of the bindings should have added all the directories containing the dependant DLL’s to config.py its possible that one has been missed or you are using a less common configuration. In these cases you will have to\n\nfirst track down which DLL’s are missing (see this guide for assistance) and then\npermanantly add the directory containing them to your installation by modifying the contents of PATH_TO_PYTHON_DIST/Lib/site-packages/cv2/config.py.\n\ne.g. If you built OpenCV against CUDA 12.1 and your own version of the FFMpeg libraries (-DOPENCV_FFMPEG_USE_FIND_PACKAGE=ON) instead of the provided opencv_videoio_ffmpegxxx_64.dll the contents of config.py should look like\nimport os\n\nBINARIES_PATHS = [\n    os.path.join('D:/build/opencv/install', 'x64/vc17/bin'),\n    os.path.join(os.getenv('CUDA_PATH', 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1'), 'bin')\n    os.path.join(`D:/ffmpeg/bin`)\n] + BINARIES_PATHS"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#manually-installing-opencv-python-bindings",
    "href": "accelerate_opencv_cuda_python.html#manually-installing-opencv-python-bindings",
    "title": "Accelerate OpenCV on Windows with CUDA and Python",
    "section": "Manually installing OpenCV Python bindings",
    "text": "Manually installing OpenCV Python bindings\nIf you have downloaded the pre-built binaries or are having issues with the automatic installation then you can manually install the python bindings following the steps below:\n\nCopy\n\nPATH_TO_BUILD_DIR/lib/python3/cv2.cpxx-win_amd64.pyd\n\nto\n\nPATH_TO_PYTHON_DIST/Lib/site-packages/cv2.cpxx-win_amd64.pyd\n\nDetermine the paths to the directories containing any dependant shared libraries (see here for assistance).\nAdding the locations from (2) by calling os.add_dll_directory() for each one before importing the OpenCV python module. e.g. If you have followed the guide exactly this will just be the directories containing the OpenCV and Nvidia shared libaries, which you would add as\nimport os\nos.add_dll_directory('C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\vxx.x\\\\bin')\nos.add_dll_directory('PATH_TO_BUILD_DIR/bin')\nbefore calling\nimport cv2 as cv"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#tips",
    "href": "accelerate_opencv_cuda_python.html#tips",
    "title": "Accelerate OpenCV on Windows with CUDA and Python",
    "section": "Tips",
    "text": "Tips\nIf you change your CMake arguments and you start getting build errors clean everything\nRemove all optional CUDA modules. This is useful if you only want to use the CUDA backend for the DNN module and will significantly reduce compilation time and size of opencv_worldxxx.dll.\n-DBUILD_opencv_cudaarithm=OFF -DBUILD_opencv_cudabgsegm=OFF -DBUILD_opencv_cudafeatures2d=OFF -DBUILD_opencv_cudafilters=OFF -DBUILD_opencv_cudaimgproc=OFF -DBUILD_opencv_cudalegacy=OFF -DBUILD_opencv_cudaobjdetect=OFF -DBUILD_opencv_cudaoptflow=OFF -DBUILD_opencv_cudastereo=OFF -DBUILD_opencv_cudawarping=OFF -DBUILD_opencv_cudacodec=OFF\n```{.python .code-overflow-wrap}\nset \"openCvSource=PATH_TO_OPENCV_SOURCE\"\nset \"openCVExtraModules=PATH_TO_OPENCV_CONTRIB_MODULES\"\nset \"openCvBuild=%openCvSource%\\build\"\nset \"buildType=Release\"\nset \"generator=Visual Studio 16 2019\"\n```\n\nTroubleshooting\n\n\n\n\n\n\nTip\n\n\n\nIf you don’t want to build python bindings then simply remove…\n\n\ndifferent version of python\nI have seen lots of guides including instructions to download and use git to get the source files, however this is a completely unnecessary step. If you are a developer and you don’t already have git installed and configured then I would assume there is a good reason for this and I would not advise installing it just to build OpenCV.\n\n\n\n\n\n\nNote\n\n\n\n\n\nBefore building you may want to ensure that your GPU has decoding support by refering to Nvidia Video Decoder Support Matrix\n\n\n\nwhere PATH_TO_OPENCV_SOURCE is the root of the OpenCV files you downloaded or cloned (the directory containing 3rdparty, apps, build, etc., PATH_TO_OPENCV_CONTRIB_MODULES is the path to the modules directory inside the opencv-contrib repo (the directory containing cudaarithm, cudabgsegm, etc), PATH_TO_BUILD_DIR is the path to the directory where the build files should go and PATH_TO_PYTHON_DIST is the directory where miniconda was installed and PYTHON_VERSION is the concatination of the major and minor version of your python install, e.g. for Python 3.10.10 PYTHON_VERSION=310.\nThis will build OpenCV with CUDA including and the corresponding tests and examples for verifcation. Additionally if the Nvidia Video Codec SDK or cuDNN are installed the corresponding modules will automatically be included\n\n1. Set the location of the source files and build directory, by entering the text shown below, first setting `PATH_TO_OPENCV_SOURCE` to the root of the OpenCV files you downloaded or cloned (the directory containing 3rdparty, apps, build, etc.) and `PATH_TO_OPENCV_CONTRIB_MODULES` to the **modules** directory inside the &lt;a href=\"https://github.com/opencv/opencv_contrib/tree/master/modules\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;opencv-contrib repo&lt;/a&gt; (the directory containing cudaarithm, cudabgsegm, etc).\n\n```{.python .code-overflow-wrap}\nset \"openCvSource=PATH_TO_OPENCV_SOURCE\"\nset \"openCVExtraModules=PATH_TO_OPENCV_CONTRIB_MODULES\"\nset \"openCvBuild=%openCvSource%\\build\"\nset \"buildType=Release\"\nset \"generator=Visual Studio 16 2019\"\n```\n\n```{.python .code-overflow-wrap}\nset \"openCvSource=PATH_TO_OPENCV_SOURCE\"\nset \"openCVExtraModules=PATH_TO_OPENCV_CONTRIB_MODULES\"\nset \"openCvBuild=%openCvSource%\\build\"\nset \"buildType=Release\"\nset \"generator=Visual Studio 16 2019\"\n```"
  },
  {
    "objectID": "how2.html",
    "href": "how2.html",
    "title": "Tutorials",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "howto.html",
    "href": "howto.html",
    "title": "How To lala",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "opencv_cuda_performance.html",
    "href": "opencv_cuda_performance.html",
    "title": "OpenCV CUDA Performance Comparisson (Nvidia vs Intel)",
    "section": "",
    "text": "In this post I am going to use the OpenCV’s performance tests to compare the CUDA and CPU implementations. The idea, is to get an indication of which OpenCV and/or Computer Vision algorithms, in general, benefit the most from GPU acceleration, and therefore, under what circumstances it might be a good idea to invest in a GPU.\n\n\nTest Setup\n\n\nGPU Specifications\n\n\nCPU Specifications\n\n\nBenchmark Results\n\n\nAverage OpenCV GPU Performance Increase\n\n\nTop 20 OpenCV GPU Functions\n\n\nBottom 20 OpenCV GPU Functions\n\n\n20 Slowest OpenCV CPU Functions\n\n\n\n\n\n\nTest Setup\n\n\n\nSoftware: OpenCV 3.4 compiled on Visual Studio 2017 with CUDA 9.1, Intel MKL with TBB, and TBB. To generate the CPU results I simply ran the CUDA performance tests with CUDA disabled, so that the fall back CPU functions were called, by changing the following\n#define PERF_RUN_CUDA()  false //::perf::GpuPerf::targetDevice()\non line 228 of\nmodules\\ts\\include\\opencv2\\ts\\ts_perf.hpp.\nThe performance tests cover 104 of the OpenCV functions, with each function being tested for a number of different configurations (function arguments). The total number of different CUDA performance configurations/tests which run successfully are 6031, of which only 5300 configurations are supported by both the GPU and CPU.\n\n\nHardware:  Four different hardware configurations were tested, consisting of 3 laptops and 1 desktop, the CPU/GPU combinations are listed below:\n\n\nCPU: i5-4120U, GPU: 730m (laptop)\n\n\nCPU: i5-5200U, GPU: 840m (laptop)\n\n\nCPU: i7-6700HQ, GPU: GTX 980m (laptop)\n\n\nCPU: i5-6500, GPU: GTX 1060 (desktop)\n\n\n\n\n\n\nGPU Specifications\n\nThe GPU’s tested comprise three different micro-architectures, ranging from a low end laptop (730m) to a mid range desktop (GTX 1060) GPU. The full specifications are shown below, where I have also included the maximum theoretical speedup, if the OpenCV function were bandwidth or compute limited. This value is just included to give an indication of what should be possible if architectural improvements, SM count etc. don’t have any impact on performance. In “general” most algorithms will be bandwidth limited implying that the average speed up of the OpenCV functions could be somewhere between these two values. If you are not familiar with this concept then I would recommend watching Memory Bandwidth Bootcamp: Best Practices, Memory Bandwidth Bootcamp: Beyond Best Practices and Memory Bandwidth Bootcamp: Collaborative Access Patterns by Tony Scudiero for a good overview.\n\n\n\nCPU Specifications\n\nThe CPU’s tested also comprise three different micro-architectures, ranging from a low end laptop dual core (i5-4120U) to a mid range desktop quad core (i5-6500) CPU. The full specifications are shown below, where I have again included the maximum theoretical speedup depending on whether the OpenCV functions are limited by the CPU bandwidth or clock speed (I could not find any Intel published GFLOPS information).\n\n\n\nBenchmark Results\n\nThe results for all tests are available here, where you can check if a specific configuration benefits from an improvement in performance when moved to the GPU.\nTo get an overall picture of the performance increase which can be achieved from using the CUDA functions over the standard CPU ones, the speedup of each CPU/GPU over the least powerful CPU (i5_4210U), is compared. The below figure shows the speedup averaged over all 5300 tests (All Configs). Because the average speedup is influenced by the number of different configurations tested per OpenCV function, two additional measures are also shown (which only consider one configuration per function) on the below figure:\n\n\nGPU Min - the average speedup, taken over all OpenCV functions for the configuration where the GPU speedup was smallest.\n\n\nGPU Max - the average speedup, taken over all OpenCV functions for the configuration where the GPU speedup was greatest.\n\n\n \nThe results demonstrate that the configuration (function arguments), makes a massive difference to the CPU/GPU performance. That said even the slowest configurations on the slowest GPU’s are in the same ball park, performance wise, as the fastest configurations on the most powerful CPU’s in the test. This combined with a higher average performance for all GPU’s tested, implies that you should nearly always see an improvement when moving to the GPU, if you have several OpenCV functions in your pipeline (as long as you don’t keep moving your data to and from the GPU), even if you are using a low end two generation old laptop GPU (730m).\nNow lets examine some individual OpenCV functions. Because each function has many configurations, for each function the average execution time over all configurations tested, is used to calculate the speedup over the i5-4120U. This will provides a guide to the expected performance of a function irrespective of the specific configuration. The next figure shows the top 20 functions where the GPU speedup, was largest. It is worth noting that the speedup of the GTX 1060 over all of the CPU’s is so large that it has to be shown on a log scale.  \n Next, the bottom 20 functions where the GPU speedup, was smallest. \nThe above figure demonstrates that, although the CUDA implementations are on average much quicker, some functions are significantly quicker on the CPU. Generally this is due to the function using the Intel Integrated Performance Primitives for Image processing and Computer Vision (IPP-ICV) and/or SIMD instructions. That said the above results also show, that some of these slower functions, do benefit from the parallelism of the GPU, but a more powerful GPU is required to leverage this.\nFinally lets examine which OpenCV functions took the longest. This is importanti f you are using one of these functions, as you may consider calling its CUDA counterpart, even if it is the only OpenCV function you need. The below figure contains the execution time for the 20 functions which took the longest on the i5-4120U, again this has to be shown on a log scale because the GPU execution time is much smaller than the CPU execution time.\n \nGiven the possible performance increases shown in the results, if you were performing mean shift filtering with OpenCV, on a laptop with only low end i5-4120U, the execution time of nearly 7 seconds may encourage you to upgrade your hardware. From the above it is clear that it is much better to invest in a tiny GPU (730m) which will reduce your processing time by a factor of 10 to a more tolerable 0.6 seconds, or a mid range GPU (GTX 1060), reducing your processing time by a factor of 100 to 0.07 seconds, rather than a mid range i7 which will give you less than a 30% reduction.\nTo conclude I would just reiterate that, the benefit you will get from moving your processing to the GPU with OpenCV will depend on the function you call and configuration that you use, in addition to your processing pipeline. That said from, what I have observed, on average the CUDA functions are much much quicker than their CPU counterparts. Please let me know if there are any mistakes in my results and/or analysis."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "OpenCV Guides",
    "section": "",
    "text": "“ImportError: DLL load failed while importing cv2: The specified module could not be found.”\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAccelerate OpenCV on Windows with CUDA and Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nOverview\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials1.html",
    "href": "tutorials1.html",
    "title": "Tutorials",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "tutorials2.html",
    "href": "tutorials2.html",
    "title": "Tutorials",
    "section": "",
    "text": "About this site"
  }
]