[
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html",
    "title": "ImportError: DLL load failed while importing cv2: The specified module could not be found.",
    "section": "",
    "text": "If your on Windows using python &gt;= 3.8 having built OpenCV &gt;= 4.6 from source and your seeing the above error when calling import cv2, this short guide should help solve your problem.\nThe guide assumes that you have either installed the python bindings during the build process (not recommended) or manually copied cv2.cpxx-win_amd64.pyd to your distributions site-packages directory (e.g. C:\\Users\\&lt;USER&gt;\\mambaforge\\Lib\\site-packages).\nSo what’s the issue? Although the message is quite explicit regarding the cause, it doesn’t really help with finding a solution. In a nutshell python has found cv2.cpxx-win_amd64.pyd, tried and then failed to load it because a it can’t find a dependant shared library. Now the advice I have seen online regarding this is to dig out trusty dependancy walker, load the cv2.cpxx-win_amd64.pyd and see which dependencies the system can’t find.\nNow this is solid advice if we had a C++ application and/or we were using python &lt; 3.8 (which uses the system/user path for dll resolution), however we are not, so even if dependency walker can’t detect any problems we may still be facing the above error.\nThe good news is there is an easy fix if you know where the missing DLL’s are and only slightly more involved if you don’t as long as you have access to the missing DLL’s on your system."
  },
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#manually-adding-filter-entries",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#manually-adding-filter-entries",
    "title": "ImportError: DLL load failed while importing cv2: The specified module could not be found.",
    "section": "Manually Adding Filter Entries",
    "text": "Manually Adding Filter Entries\nSince we only want to view shared libaries which are accessed by the python.exe process, we can add the following filters to make our lives easier:\n\nProcess Name -&gt; is -&gt; python.exe\nOperation -&gt; is -&gt; CreateFile\nResult -&gt; is -&gt; NAME NOT FOUND\nResult -&gt; is -&gt; SUCCESS\nPath -&gt; contains -&gt; .dll\nPath -&gt; contains -&gt; .pyd (not striclty necessary, if this was missing the error would be “ModuleNotFoundError: No module named ‘cv2’” but its a shared library so why not)\n\nYour filter should now resemble the below.\n\n\n\nProcess Monitor Filter\n\n\nBefore continuing it is advisable to close any other python proceses as the output from these will pollute the main window.\nNow start python and before typing import cv2, press the clear button (red trash can) in process monitor to clear any output generated during python’s initialization.\n\nimport cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\nProcess Monitor failed to locate cv2.cp39-win_amd64.pyd\n\n\nBecause I have reset the python DLL search path on running import cv2 I get the above output in process monitor which shows that we successfully found cv2.cp39-win_amd64.pyd (otherwise we would see the “ModuleNotFoundError: No module named ‘cv2’” error) however it also shows several attempts have been made to locate opencv_img_hash_470.dll and opencv_world470.dll without success.\nAs before we add the directory containing these to the python DLL search path.\n\nimport os\nos.add_dll_directory(\"D:\\\\build\\\\opencv\\\\4_7_0\\\\cuda_12_D\\\\bin\")\nimport cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\nProcess Monitor failed to locate nppc64_12.dll\n\n\nNow process monitor shows that opencv_img_hash_470.dll and opencv_world470.dll were located successfully after a few attempts however we are missing nppc64_12.dll which is part of the CUDA SDK. As before if we add the CUDA SDK binary directory to the python DLL search path the call to import cv2 will be successful. If however we were still seeing the same error we could simply repeat the process, that is examine the output in process monitor and add the directories containing the missing DLL’s to the python DLL search path."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html",
    "href": "nbs/opencv4-cuda-streams.html",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "",
    "text": "Since Aug 2018 the OpenCV CUDA API has been exposed to python. To get the most from this new functionality you need to have a basic understanding of CUDA (most importantly that it is data not task parallel) and its interaction with OpenCV. Below I have tried to introduce these topics with an example of how you could optimize a toy video processing pipeline. The actual functions called in the pipeline are not important, they are simply there to simulate a common processing pipeline consisting of work performed on both the host (CPU) and device (GPU).\nThis guide is taken from a Jupyter Notebook which can be cloned from here. The procedure is as follows, following some quick initialization, we start with a base implementation on both the CPU and GPU to get a baseline result. We then proceed to incrementally improve the implementation by using the information provided by the Nvidia Visual Profiler.\nOn a laptop RTX 2080 paired with an i7-8700 the final CUDA incarnation resulted in a speed up of ~30x and ~10x over the naive CPU and GPU implementations."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#cpu",
    "href": "nbs/opencv4-cuda-streams.html#cpu",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "CPU",
    "text": "CPU\n\n\nExpand to inspect the source code\nbgmog2 = cv.createBackgroundSubtractorMOG2()\ndef ProcFrameCPU0(frame,lr,store_res=False):\n    frame_big = cv.resize(frame,(cols_big,rows_big))\n    fg_big = bgmog2.apply(frame_big,learningRate = lr)\n    fg_small = cv.resize(fg_big,(frame.shape[1],frame.shape[0]))\n    if(store_res):\n        cpu_res.append(np.copy(fg_small))\ncpu_res = []\ncpu_time_0, n_frames = ProcVid0(partial(ProcFrameCPU0,store_res=check_res),lr)\nprint(f'CPU 0 (naive): {n_frames} frames, {cpu_time_0:.2f} ms/frame')\n\n\nCPU 0 (naive): 100 frames, 30.05 ms/frame"
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#gpu",
    "href": "nbs/opencv4-cuda-streams.html#gpu",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "GPU",
    "text": "GPU\n\n\nExpand to inspect the source code\nbgmog2_device = cv.cuda.createBackgroundSubtractorMOG2()\ndef ProcFrameCuda0(frame,lr,store_res=False):\n    frame_device.upload(frame)\n    frame_device_big = cv.cuda.resize(frame_device,(cols_big,rows_big))\n    fg_device_big = bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null())\n    fg_device = cv.cuda.resize(fg_device_big,frame_device.size())\n    fg_host = fg_device.download()\n    if(store_res):\n        gpu_res.append(np.copy(fg_host))\ngpu_res = []\ngpu_time_0, n_frames = ProcVid0(partial(ProcFrameCuda0,store_res=check_res),lr)\nprint(f'GPU 0 (naive): {n_frames} frames, {gpu_time_0:.2f} ms/frame')\nprint(f'Speedup over CPU: {cpu_time_0/gpu_time_0:.2f}')\n\n\nGPU 0 (naive): 100 frames, 3.92 ms/frame\nSpeedup over CPU: 7.67\n\n\n\n\nAnalysis\n\n\n\n\nBase Implementation\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe output gpu_time_0 from above is the average amount of time to process each frame, recorded on the host. This will be referred to as the frame time and is the value that we want to reduce. In order to achieve this we need to investigate what is actually occurring on the host and device for each frame. Luckily the Nvidia provides a useful visual tool for this, the Nvidia Visual Profiler.\nThe image above shows the Nvidia Visual Profiler output from processing 2 of the 100 frames. Important things to be aware of here are:\n\nThe runtime API calls in brown which in this example represent the time the host (CPU) spends waiting for the device (GPU) calls to return.\nThe remaining blocks which show the time spent on the device. This is split according to the operation (kernel, memset, MemCpy(HtoD), MemCpy(DtoH)) as well as by the CUDA stream which the operations are issued to. In this case everything is issued to the Default stream.\nThe 0.93ms gap in between the blocks of runtime API calls represents the time spent executing code on the host, here that is the time taken for OpenCV to read and decode each video frame, frame = cap.read().\nIn this naive implementation all device calls from the host are synchronous and as a result the difference between (1) and (2) can be interpreted as periods where no useful work is being performed on either the host or the device. The host is blocking waiting for the device to return and the device is also idle, allocating or freeing memory.\n\nFrom now on for convenience, for a single frame, I will refer to 1), 2) and 3) as the runtime API time, device time, host time respectively. As shown the profiler output, the current runtime API time and host time are ~2.38ms and ~0.93ms.\nTaking (1) and (4) into account from left to right the output from the profiler can be mapped to the python calls as:\n\n(1217.62ms-1220ms) proc_frame_func(frame,lr): calls to the device to process the first frame (~2.38ms)\n(1220ms-1220.93ms) frame = cap.read(): read and decode the second video frame on the host (~0.93ms)\n(1220.93ms-) proc_frame_func(frame,lt): calls to the device to process the second frame\n\nClearly from the gaps described in (4) a lot of time is wasted waiting for the device calls to return, and as the host time does not overlap the device time, there is a lot of room for improvement.\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nThe main causes of (4) are the blocking calls to both\n\ncudaMallocPitch() - OpenCV in python automatically allocates any arrays (NumPy or GpuMat) which are returned from a function call. That is on every iteration\nret, frame = cap.read()\ncauses memory for the NumPy array frame to be allocated and destroyed on the host and\nframe_device_big = cv.cuda.resize(frame_device,(cols_big,rows_big))\nfg_device_big = bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null())\nfg_device = cv.cuda.resize(fg_device_big,frame_device.size())\ncauses memory for frame_device_big, fg_device_big and fg_device to be allocated and destroyed on the device.\ncudaDeviceSynchronise() - if you don’t explicitly pass in a CUDA stream to an OpenCV CUDA function, the default stream will be used and cudaDeviceSynchronize() will be called before the function exits, stalling the GPU every time.\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nAddress the unnecessary calls to cudaMallocPitch(), by pre-allocating any output arrays and passing them as input arguments."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#cpu-1",
    "href": "nbs/opencv4-cuda-streams.html#cpu-1",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "CPU",
    "text": "CPU\n\n\nExpand to inspect the source code\nclass ProcFrameCpu1:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.createBackgroundSubtractorMOG2()\n        self.frame = np.empty((rows_small,cols_small,3),np.uint8)\n        self.frame_big = np.empty((rows_big,cols_big,3),np.uint8)\n        self.fg_big = np.empty((rows_big,cols_big),np.uint8)\n        self.fg_small = np.empty((rows_small,cols_small),np.uint8)\n        \n    def ProcessFrame(self,lr):\n        cv.resize(self.frame,(self.cols_big,self.rows_big),self.frame_big)\n        self.bgmog2.apply(self.frame_big,self.fg_big,learningRate = lr)\n        cv.resize(self.fg_big,(self.cols_small,self.rows_small),self.fg_small)\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_small))\n        \n    def Frame(self):\n        return self.frame\n    \ncap = cv.VideoCapture(vidPath)\nif (cap.isOpened()== False): \n  print(\"Error opening video stream or file\")\nret, frame = cap.read()\ncap.release()\nrows_small,cols_small = frame.shape[:2]\nproc_frame_cpu1 = ProcFrameCpu1(rows_small,cols_small,rows_big,cols_big,check_res)\ncpu_time_1, n_frames = ProcVid1(proc_frame_cpu1,lr)\nprint(f'CPU 1 (pre-allocation): {n_frames} frames, {cpu_time_1:.2f} ms/frame')\nprint(f'Speedup over CPU baseline: {cpu_time_0/cpu_time_1:.2f}')\n\n\nCPU 1 (pre-allocation): 100 frames, 27.76 ms/frame\nSpeedup over CPU baseline: 1.08"
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#gpu-1",
    "href": "nbs/opencv4-cuda-streams.html#gpu-1",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "GPU",
    "text": "GPU\n\n\nExpand to inspect the source code\nclass ProcFrameCuda1:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.frame = np.empty((rows_small,cols_small,3),np.uint8)\n        self.frame_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3)\n        self.frame_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3)        \n        self.fg_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1)\n        self.fg_device_big.setTo(0)\n        self.fg_device = cv.cuda_GpuMat(np.shape(frame)[0],np.shape(frame)[1],cv.CV_8UC1)\n        self.fg_host = np.empty((rows_small,cols_small),np.uint8)\n        \n    def ProcessFrame(self,lr):\n        self.frame_device.upload(self.frame)\n        cv.cuda.resize(self.frame_device,(cols_big,rows_big),self.frame_device_big)\n        self.bgmog2.apply(self.frame_device_big,lr,cv.cuda.Stream_Null(),self.fg_device_big)\n        cv.cuda.resize(self.fg_device_big,self.fg_device.size(),self.fg_device)\n        self.fg_device.download(self.fg_host)\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_host))\n        \n    def Frame(self):\n        return self.frame\n    \nproc_frame_cuda1 = ProcFrameCuda1(rows_small,cols_small,rows_big,cols_big,check_res)\ngpu_time_1, n_frames = ProcVid1(proc_frame_cuda1,lr)\nprint(f'GPU 1 (pre-allocation): {n_frames} frames, {gpu_time_1:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_0/gpu_time_1:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_1:.2f}')\n\n\nGPU 1 (pre-allocation): 100 frames, 1.99 ms/frame\nIncremental speedup: 1.96\nSpeedup over CPU: 13.91"
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#analysis-1",
    "href": "nbs/opencv4-cuda-streams.html#analysis-1",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nPre-allocation of return arrays\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nPre-allocating the arrays has successfully removed the calls to cudaMallocPitch() and significantly (3 frames are now processed instead of 1.5) reduced (4), the time the host spends waiting for the CUDA runtime to return control to it.\nPre-allocation on the host has also reduced the host time from ~0.93ms to ~0.57ms. The host time will now be unaffected by the remaining changes we make and can be observed to be approximately constant after each of the following optimizations.\nWe will now proceed to try and reduce the runtime API time which in this step has already been fallen from ~2.38ms to ~1.15ms.\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nAs mentioned above by not specifying a CUDA stream all calls are placed in the “Default” stream which can be seen at the bottom of the figure. This means that due to the way OpenCV is implemented following each asynchronous kernel launch there will be an internal synchronizing call to cudaDeviceSynchronize() shown below:\n1cv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big)\ncudaDeviceSynchronize()\n2bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null(),fg_device_big)\ncudaDeviceSynchronize()\n3cv.cuda.resize(fg_device_big,fg_device.size(),fg_device)\ncudaDeviceSynchronize()\n4fg_device.download(fg_host)\n\n1\n\nasynchronous kernel 1\n\n2\n\nasynchronous kernel 2\n\n3\n\nasynchronous kernel 3\n\n4\n\nsynchronous copy from device to host\n\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nPass a non default CUDA stream to each OpenCV CUDA function."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#analysis-2",
    "href": "nbs/opencv4-cuda-streams.html#analysis-2",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nReplacing the default stream\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe calls to cudaDeviceSyncronize() have now been removed, and as a result the gaps between the device calls have disappeared, further reducing the runtime API time from ~1.15ms to ~1.07ms. That said it looks like the calls to cudaDeviceSyncronize() have just been replaced by calls to cudaMemcpy2DAsync().\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nWhat has actually happened is we have tried to use asynchronous copies to and from the device without first pinning the host memory. Therefore what is shown are three asynchronous kernel launches and a synchronous copy from the device to the host, which blocks the host thread until all the previous work on the device is complete:\n    cv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big,stream=stream) async kernel 1\n    bgmog2.apply(frame_device_big,lr,stream,fg_device_big) acync kernel 2\n    cv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3\n    fg_device.download(stream,fg_host) synchronous copy\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nPin the host memory."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#analysis-3",
    "href": "nbs/opencv4-cuda-streams.html#analysis-3",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation (1)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe output is now more intuitive, that said all that we have done is replace the calls to cudaDeviceSyncronize() with calls to cudaStreamSyncronize().\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nWe are issuing asynchronous calls to the device and then immediately waiting on the host for them to complete.\ncv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big,stream=stream) async kernel 1\nbgmog2.apply(frame_device_big,lr,stream,fg_device_big) async kernel 2\ncv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3\nfg_device.download(stream,fg_host.array) async copy DtoH\nstream.waitForCompletion() block until kernel 1-3 and copy have finished\nWhat we really want to do is overlap host and device computation by issuing asynchronous calls to the device and then performing processing on the host, before waiting for the asynchronous device calls to return. For two frames this would be:\nframe_device.upload(frame[0].array,stream) async copy HtoD, frame 0\ncv.cuda.resize(frame_device,(n_cols_big,n_rows_big),frame_device_big,stream=stream) async kernel 1, frame 0\nbgmog2.apply(frame_device_big,lr,stream,fg_device_big) async kernel 2, frame 0\ncv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3, frame 0\nfg_device.download(stream,fg_host.array) async copy DtoH, frame 0\nret,_ = cap.read(frame[1].array) host read frame 1\nstream.waitForCompletion() block until kernel 1-3 and copy have finished for frame 0\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nMove the position of the synchronization point to after a new frame has been read as described above. To do this we also need to increase the number of host frame containers to two because moving the sync point means frame 0 may still be in the process of being uploaded to the device when we read frame 1. That is, when we call\nret,_ = cap.read(frame[1].array)\nwe have not synced, and we have no way to know if the previous call to\nframe_device.upload(frame[0].array,stream)\nhas finished, hence we need to read to frame[1].array and not frame[0].array."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#analysis-4",
    "href": "nbs/opencv4-cuda-streams.html#analysis-4",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation (2)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nAt first glance changing the synchronization point does not appear to have done anything the cudaStreamSynchronize() (stream.waitForCompletion()) still starts at the point just before the frame is processed on the device. On closer inspection we can see that the runtime API time (~1.5ms) now begins much earlier than the device time (~0.8ms) and as we intended overlaps the host time. That said we are not seeing any host/device processing overlap, so whats going on?\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nThis is most likely to be because we are working on Windows where the GPU is a Windows Display Driver Model device. See below for more details.\n\nCUDA driver has a software queue for WDDM devices to reduce the average overhead of submitting command buffers to the WDDM KMD driver\n\nThis would cause all the device calls from the previous frame to be queued and then issued when we call stream.waitForCompletion() and could explain the profiler output.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nTest the hypothesis by forcing the CUDA driver to dispatch all queued calls by issuing a call to stream.queryIfComplete() as shown below.\nframe_device.upload(frames_in[0].array, stream) async copy HtoD, frame 0\ncv.cuda.resize(frame_device,(n_cols_big,n_rows_big),frame_device_big,stream=stream) async kernel 1, frame 0\nbgmog2.apply(frame_device_big, lr, stream, fg_device_big ) async kernel 2, frame 0\ncv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3, frame 0\nfg_device.download(stream,fg_host.array) async copy DtoH, frame 0\nstream.queryIfComplete() force WDDM to dispatch any queued device calls\nret,_ = cap.read(frame[1].array) host read frame 1\nstream.waitForCompletion() block until kernel 1-3 and copy have finished for frame 0"
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#analysis-5",
    "href": "nbs/opencv4-cuda-streams.html#analysis-5",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation (3)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nIt appears as though the WDDM driver was at fault, by including the extra call to stream.queryIfComplete() we have finally overlapped the processing on the host and device. This can be observed in the profiler output where the host time (~0.62ms), overlaps the device time (~0.79ms) in Stream 2017. Notice also that there are gaps between the blocks of device time in Stream 2017 with the runtime API time (~1.07ms) still starting sometime before the device time and ending exactly after the Memcpy (DtoH) (fg_device.download(stream,fg_host.array)).\nMost importantly the device is almost saturated with only the small gap (~0.2ms) in between each block representing the device time for each frame in Stream 2017. So what is causing this small gap?\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nThe device is stalling.\nAs already mentioned the host time cannot be changed. Additionally from the profiler output it is clear that the host time (~0.62ms) is less than the device time (~0.79ms).\nThat is given the processing pipeline below;\n\nprocess frame 0 on the device ~0.79ms (copy frame 0 to the device execute kernel 1-3 and copy back to the host)\nret,_ = cap.read(frame[1].array) ~0.62ms (read frame 1 on the host)\nstream.waitForCompletion() block for (~0.17ms = 0.79ms-0.62ms) until processing for frame 0 has finished\n\nstream.waitForCompletion() will on average cause the host to wait ~0.17ms for the device processing to finish. This can be observed in the profiler output by the length of cudaStreamchronize() which for each frame ends exactly following the Memcpy(DtoH). Unfortunately this wait stalls the device because it has no work to perform until more calls are issued by the host, which in this case does not occur until after the call to stream.waitForCompletion(). If only there was a way to issue work to the device in advance of stream.waitForCompletion(), which will continue to be performed afterwards.\nFortunately there is by using multiple streams, each processing a single frame at a time. This allows us to issue commands in advance, to process frame 1 before we start the wait on the host for frame 0, shown below\n\nret,_ = cap.read(frame[0].array) host read frame 0\nProcess frame 0 in stream 0\nret,_ = cap.read(frame[1].array) host read frame 1\nProcess frame 1 in stream 1\nret,_ = cap.read(frame[2].array) host read frame 2\nstream[0].waitForCompletion() block until (2) the processing for frame 0 has finished, allowing the device to continue with (4)\nProcess frame 2 in stream 0\nret,_ = cap.read(frame[0].array) host read frame 3\nstream[1].waitForCompletion() block until (4) the processing for frame 1 has finished, allowing the device to continue with (7)\n…\n\nNotice that when stream[0].waitForCompletion() is called the device has Process frame 1 in stream 1 already queued up in stream 1 meaning that the wait on the host should not cause a stall on the device.\nNote: Using multiple streams in this way will add additional latency and is not going to be suitable for real time processing, that said the additional latency in most real world cases will be tolerable and worth the reduction in processing time.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nUse multiple streams."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#analysis-6",
    "href": "nbs/opencv4-cuda-streams.html#analysis-6",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation - multiple streams\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe device is now completely saturated with memory operations overlapping kernel executions in Streams 2418 and 2419. Additionally the host and device time completely overlap each other. By saturating the device and overlapping host and device computation we have probably reached the limit of the optimizations we can apply to this particular toy problem.\nNotice that as a result of the kernel/memory overlap the average device time is no longer equal to the average amount of time to process a frame on the device (streamed device time). In fact because of kernel/memory and host/device overlap the average device time should now be greater than both the average streamed device and frame time.\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nIf the above assumption is correct we should be able to see this effect by using device timers to get a more accurate value for the average device time.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\n\nUse device timers to get the average device time. Unfortunately this introduces some overhead so we will have to compare this to the average time required to process each frame calculated without the device timers. This may mean that we may not see the difference that we expect.\nCalculate the theoretical average time to process each frame on the host and then the device without overlap (host time + device time), to see the gain from host/device and kernel/memory overlap.\nCalculate the average wasted time on the host (streamed device time - host time) time where the host could be performing useful operations without increasing the average processing time)."
  },
  {
    "objectID": "nbs/opencv4-cuda-streams.html#analysis-7",
    "href": "nbs/opencv4-cuda-streams.html#analysis-7",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nIt appears that we gained 0.04 ms/frame from the kernel/memory processing overlap on the device. Unfortunately we cannot say this for sure because the times compared here are from two separate runs due to the device timer overhead. That said, the implication is that our interpretation of the kernel/memory overlap seen in the Nvidia Visual Profiler is correct.\nThe total processing which needs to be performed on the host and device takes and average of 1.71 ms/frame which is 0.74 ms/frame greater than our final implementation, demonstrating the importance of using asynchronous device calls and CUDA streams.\nWe have 0.26 ms/frame to spare on the host which we can make use of without affecting the average frame time of 0.97 ms/frame."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Location for storing useful guides and notebooks"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#building-opencv-with-cuda-using-visual-studio-solution-files-from-the-command-prompt-cmd",
    "href": "accelerate_opencv_cuda_python.html#building-opencv-with-cuda-using-visual-studio-solution-files-from-the-command-prompt-cmd",
    "title": "Build OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows",
    "section": "Building OpenCV with CUDA using Visual Studio solution files from the command prompt (cmd)",
    "text": "Building OpenCV with CUDA using Visual Studio solution files from the command prompt (cmd)\nThe following steps will build the opencv_worldxxx.dll using NVIDIA’s recommended settings for future hardware compatibility. This does however have two drawbacks, first the build can take several hours to complete and second, the shared library can be over 1GB depending on the configuration that you choose. To find out how to reduce both the compilation time and size of opencv_worldxxx.dll read choosing a suitable CUDA compute capability first and then continue as below. Additionally to reduce the build time futher you can use the Ninja build system, see building OpenCV with the ninja build system to reduce the build time.\n\nOpen windows command prompt, type cmd in the Windows search bar.\n\n\n\n\n\nPaste the below into to the command prompt and press Enter.\n\n\n\n\nAll CUDA modulescuDNN onlyWithout Python bindingsWithout CUDANinja\n\n\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -B\"PATH_TO_BUILD_DIR\" -H\"PATH_TO_OPENCV_SOURCE\" -DOPENCV_EXTRA_MODULES_PATH=\"PATH_TO_OPENCV_CONTRIB_MODULES\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DWITH_CUFFT=ON -DCUDA_ARCH_PTX=9.0 -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=PATH_TO_PYTHON_DIST/include -DPYTHON3_LIBRARY=PATH_TO_PYTHON_DIST/libs/python%pyVer%.lib -DPYTHON3_EXECUTABLE=PATH_TO_PYTHON_DIST/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=PATH_TO_PYTHON_DIST/lib/site-packages/numpy/core/include -DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/\n\n\nIf you just want to CUDA accelerate the DNN module and are not interested in building the rest of the CUDA modules this will significantly reduce compilation time and size of opencv_worldxxx.dll.\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -B\"PATH_TO_BUILD_DIR\" -H\"PATH_TO_OPENCV_SOURCE\" -DOPENCV_EXTRA_MODULES_PATH=\"PATH_TO_OPENCV_CONTRIB_MODULES\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DWITH_CUFFT=ON -DCUDA_ARCH_PTX=9.0 -DBUILD_opencv_cudaarithm=OFF -DBUILD_opencv_cudabgsegm=OFF -DBUILD_opencv_cudafeatures2d=OFF -DBUILD_opencv_cudafilters=OFF -DBUILD_opencv_cudaimgproc=OFF -DBUILD_opencv_cudalegacy=OFF -DBUILD_opencv_cudaobjdetect=OFF -DBUILD_opencv_cudaoptflow=OFF -DBUILD_opencv_cudastereo=OFF -DBUILD_opencv_cudawarping=OFF -DBUILD_opencv_cudacodec=OFF -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=PATH_TO_PYTHON_DIST/include -DPYTHON3_LIBRARY=PATH_TO_PYTHON_DIST/libs/python%pyVer%.lib -DPYTHON3_EXECUTABLE=PATH_TO_PYTHON_DIST/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=PATH_TO_PYTHON_DIST/lib/site-packages/numpy/core/include -DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/\n\n\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -B\"PATH_TO_BUILD_DIR\" -H\"PATH_TO_OPENCV_SOURCE\" -DOPENCV_EXTRA_MODULES_PATH=\"PATH_TO_OPENCV_CONTRIB_MODULES\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DWITH_CUFFT=ON -DCUDA_ARCH_PTX=9.0\n\n\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -B\"PATH_TO_BUILD_DIR\" -H\"PATH_TO_OPENCV_SOURCE\" -DOPENCV_EXTRA_MODULES_PATH=\"PATH_TO_OPENCV_CONTRIB_MODULES\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=PATH_TO_PYTHON_DIST/include -DPYTHON3_LIBRARY=PATH_TO_PYTHON_DIST/libs/python%pyVer%.lib -DPYTHON3_EXECUTABLE=PATH_TO_PYTHON_DIST/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=PATH_TO_PYTHON_DIST/lib/site-packages/numpy/core/include -DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/\n\n\nFor details see decreasing the build time with Ninja\n\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -B\"PATH_TO_BUILD_DIR\" -H\"PATH_TO_OPENCV_SOURCE\" -DOPENCV_EXTRA_MODULES_PATH=\"PATH_TO_OPENCV_CONTRIB_MODULES\" -G\"Ninja\" -DCMAKE_BUILD_TYPE=Release -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DWITH_CUDA=ON -DWITH_CUBLAS=ON -DCUDA_ARCH_PTX=8.6 -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=PATH_TO_PYTHON_DIST include -DPYTHON3_LIBRARY=PATH_TO_PYTHON_DIST/libs/python%pyVer%.lib -DPYTHON3_EXECUTABLE=PATH_TO_PYTHON_DIST/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=PATH_TO_PYTHON_DIST/lib/site-packages/numpy/core/include -DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/ -DOPENCV_SKIP_PYTHON_LOADER=ON\nFollowing the configuration step the build is started with\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target INSTALL\n\n\n\nwhere\n\nPATH_TO_OPENCV_SOURCE is the root of the OpenCV files you downloaded or cloned (the directory containing 3rdparty, apps, build, etc.),\nPATH_TO_OPENCV_CONTRIB_MODULES is the path to the modules directory inside the opencv-contrib repository (the directory containing cudaarithm, cudabgsegm, etc.),\nPATH_TO_BUILD_DIR is the path to the directory where the build files should go,\nPATH_TO_PYTHON_DIST is the directory where mambaforge was installed and,\nPYTHON_VERSION is the concatination of the major and minor version of your python install, e.g. for Python 3.10.10 PYTHON_VERSION==310.\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen passing paths to CMake on the command line make sure not to terminate them using \\ as this is a special character and will cause the following arguments to be ignored. e.g. PATH_TO_OPENCV_SOURCE can be D:\\opencv or D:\\opencv/ but not D:\\opencv\\.\n\n\nThis will generate the build files for OpenCV with python bindings with CUDA acceleration including all the corresponding tests and examples for verifcation. Additionally if the Nvidia Video Codec SDK or cuDNN are installed the corresponding modules will automatically be included.\nExpand the tips below for an example of the CMake output if the configuration step is successful and how to check that output to make sure the Python bindings will be being built.\n\n\n\n\n\n\nExample of CMake Configuration Output\n\n\n\n\n\n--\n-- General configuration for OpenCV 4.7.0-dev =====================================\n--   Version control:               4.7.0-252-g88a438e542\n--\n--   Extra modules:\n--     Location (extra):            D:/repos/opencv/contrib/modules\n--     Version control (extra):     4.7.0-42-ga42b8bef\n--\n--   Platform:\n--     Timestamp:                   2023-05-03T10:21:52Z\n--     Host:                        Windows 10.0.22621 AMD64\n--     CMake:                       3.25.1\n--     CMake generator:             Visual Studio 17 2022\n--     CMake build tool:            C:/Program Files/Microsoft Visual Studio/2022/Community/MSBuild/Current/Bin/amd64/MSBuild.exe\n--     MSVC:                        1934\n--     Configuration:               Debug Release\n--\n--   CPU/HW features:\n--     Baseline:                    SSE SSE2 SSE3\n--       requested:                 SSE3\n--     Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX\n--       requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n--       SSE4_1 (18 files):         + SSSE3 SSE4_1\n--       SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n--       FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n--       AVX (8 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n--       AVX2 (36 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n--       AVX512_SKX (8 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX\n--\n--   C/C++:\n--     Built as dynamic libs?:      YES\n--     C++ standard:                11\n--     C++ Compiler:                C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.34.31933/bin/Hostx64/x64/cl.exe  (ver 19.34.31937.0)\n--     C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MD /O2 /Ob2 /DNDEBUG\n--     C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MDd /Zi /Ob0 /Od /RTC1\n--     C Compiler:                  C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.34.31933/bin/Hostx64/x64/cl.exe\n--     C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP   /MD /O2 /Ob2 /DNDEBUG\n--     C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP /MDd /Zi /Ob0 /Od /RTC1\n--     Linker flags (Release):      /machine:x64  /INCREMENTAL:NO\n--     Linker flags (Debug):        /machine:x64  /debug /INCREMENTAL\n--     ccache:                      NO\n--     Precompiled headers:         NO\n--     Extra dependencies:          cudart_static.lib nppc.lib nppial.lib nppicc.lib nppicom.lib nppidei.lib nppif.lib nppig.lib nppim.lib nppist.lib nppisu.lib nppitc.lib npps.lib cublas.lib cufft.lib -LIBPATH:C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\n--     3rdparty dependencies:\n--\n--   OpenCV modules:\n--     To be built:                 aruco barcode bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab wechat_qrcode world xfeatures2d ximgproc xobjdetect xphoto\n--     Disabled:                    -\n--     Disabled by dependency:      -\n--     Unavailable:                 alphamat cvv freetype hdf java julia matlab ovis python2 python2 sfm viz\n--     Applications:                tests perf_tests examples apps\n--     Documentation:               NO\n--     Non-free algorithms:         NO\n--\n--   Windows RT support:            NO\n--\n--   GUI:\n--     Win32 UI:                    YES\n--     VTK support:                 NO\n--\n--   Media I/O:\n--     ZLib:                        build (ver 1.2.13)\n--     JPEG:                        build-libjpeg-turbo (ver 2.1.3-62)\n--       SIMD Support Request:      YES\n--       SIMD Support:              NO\n--     WEBP:                        build (ver encoder: 0x020f)\n--     PNG:                         build (ver 1.6.37)\n--     TIFF:                        build (ver 42 - 4.2.0)\n--     JPEG 2000:                   build (ver 2.4.0)\n--     OpenEXR:                     build (ver 2.3.0)\n--     HDR:                         YES\n--     SUNRASTER:                   YES\n--     PXM:                         YES\n--     PFM:                         YES\n--\n--   Video I/O:\n--     DC1394:                      NO\n--     FFMPEG:                      YES (prebuilt binaries)\n--       avcodec:                   YES (58.134.100)\n--       avformat:                  YES (58.76.100)\n--       avutil:                    YES (56.70.100)\n--       swscale:                   YES (5.9.100)\n--       avresample:                YES (4.0.0)\n--     GStreamer:                   NO\n--     DirectShow:                  YES\n--     Media Foundation:            YES\n--       DXVA:                      YES\n--\n--   Parallel framework:            Concurrency\n--\n--   Trace:                         YES (with Intel ITT)\n--\n--   Other third-party libraries:\n--     Intel IPP:                   2021.8 [2021.8.0]\n--            at:                   D:/build/opencv/4_7_0/delete_this/3rdparty/ippicv/ippicv_win/icv\n--     Intel IPP IW:                sources (2021.8.0)\n--               at:                D:/build/opencv/4_7_0/delete_this/3rdparty/ippicv/ippicv_win/iw\n--     Lapack:                      NO\n--     Eigen:                       NO\n--     Custom HAL:                  NO\n--     Protobuf:                    build (3.19.1)\n--     Flatbuffers:                 builtin/3rdparty (23.1.21)\n--\n--   NVIDIA CUDA:                   YES (ver 12.1, CUFFT CUBLAS NVCUVID NVCUVENC)\n--     NVIDIA GPU arch:\n--     NVIDIA PTX archs:            90\n--\n--   cuDNN:                         NO\n--\n--   OpenCL:                        YES (NVD3D11)\n--     Include path:                D:/repos/opencv/opencv/3rdparty/include/opencl/1.2\n--     Link libraries:              Dynamic load\n--\n--   Python 3:\n--     Interpreter:                 C:/Users/username/mambaforge/python.exe (ver 3.9.16)\n--     Libraries:                   C:/Users/username/mambaforge/libs/python39.lib (ver 3.9.16)\n--     numpy:                       C:/Users/username/mambaforge/Lib/site-packages/numpy/core/include (ver 1.23.5)\n--     install path:                C:/Users/username/mambaforge/Lib/site-packages/cv2/python-3.9\n--\n--   Python (for build):            C:/Users/username/mambaforge/python.exe\n--\n--   Java:\n--     ant:                         NO\n--     JNI:                         NO\n--     Java wrappers:               NO\n--     Java tests:                  NO\n--\n--   Install to:                    D:/build/opencv/4_7_0/install\n-- -----------------------------------------------------------------\n--\n-- Configuring done\n-- Generating done\n-- Build files have been written to: D:/build/opencv/4_7_0\n\n\n\n\n\n\n\n\n\nVerify configuration includes Python bindings before building\n\n\n\n\n\nIf you are building the python bindings look for python3 in the To be built: section of your CMake configuration output and if its not present look for any python related errors in the output preceeding it. e.g.\n--   OpenCV modules:\n--     To be built:                 aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dpm face features2d flann fuzzy hfs highgui img_hash imgcodecs imgproc line_descriptor ml objdetect optflow phase_unwrapping photo plot python2 python3 quality reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab world xfeatures2d ximgproc xobjdetect xphoto\n\n\n\nThe OpenCV.sln solution file should now be in your PATH_TO_BUILD_DIR directory. To build OpenCV you have two options depending on you preference you can:\n\nBuild directly from the command line by simply entering the following (swaping Release for Debug to build a release version)\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target INSTALL --config Debug\nBuild through Visual Studio GUI by opening up the OpenCV.sln in Visual Studio, selecting your Configuration, clicking on Solution Explorer, expanding CMakeTargets, right clicking on INSTALL and clicking Build.\n\n\nEither approach will both build the library, install the Python bindings and copy the necessary redistributable parts to the install directory (PATH_TO_BUILD_DIR/build/install). All that is required now to run any programs compiled against these libs is to add the directory containing opencv_worldxxx.dll to you user path environmental variable.\n\nAn additionaly option you may want to include is -DCUDA_FAST_MATH=ON which compiles the CUDA kernels with the -use_fast_math option. This will however cause some of the accuracy and performace tests to fail as the floating point results will be slightly less accurate.The default installation directory for mambaforge is %userprofile%\\mambaforge.If you get the following error “CUDA : OpenCV requires enabled ‘cudev’ module from ‘opencv_contrib’” when configuring the build with CMake you have not set OPENCV_EXTRA_MODULES_PATH correctly, most likely you have set it to the root of the opencv_contrib repo and not the modules directory inside the repo.\n\n\n\n\n\nImportant\n\n\n\nBy default you have to build Release when generating python bindings, for instructions on how to build Debug see generate python bindings for a debug build\n\n\nIf everything was successful, congratulations, you now have OpenCV built with CUDA. To quickly verify that the CUDA modules are working and check if there is any performance benefit on your specific hardware see verifying OpenCV is CUDA accelerated."
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#decreasing-the-build-time-with-ninja",
    "href": "accelerate_opencv_cuda_python.html#decreasing-the-build-time-with-ninja",
    "title": "Build OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows",
    "section": "Decreasing the build time with Ninja",
    "text": "Decreasing the build time with Ninja\nThe build time for OpenCV can be reduced by more than 2x (from 2 hours to 30 mins to under an hour on an i7-8700) by utilizing the Ninja build system instead of directly generating Visual Studio solution files. The only difference you may notice is that Ninja will only produce one configuration at a time, either a Debug or Release, therefore if you don’t want to build Release (the default) the CMAKE_BUILD_TYPE has to be passed to CMake.\nNinja is installed by default if the Desktop development with C++ workload is selected when installing Visual Studio, therefore building with Ninja only requires two extra configuration steps, expand the tip below for an example of the modified command line arguments.:\n\nConfiguring Visual Studio Development tools by entering the following into the command prompt before entering the CMake command (changing Community to either Professional or Enterprise if necessary)\n\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\nTelling CMake to use Ninja instead of Visual Studio, i.e. replacing -G”Visual Studio 17 2022” with -GNinja.\n\nOnce the build files have been generated the build can be kicked off in the same way as before but this time dropping the redundant –config argument, i.e.\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target INSTALL\nfor an example fo the full command line for building a Release version of OpenCV with the Ninja build system go to the Ninja tab."
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#generate-python-bindings-for-a-debug-build",
    "href": "accelerate_opencv_cuda_python.html#generate-python-bindings-for-a-debug-build",
    "title": "Build OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows",
    "section": "Generate Python bindings for a Debug Build",
    "text": "Generate Python bindings for a Debug Build\nPython bindings cannot by default be generated for a Debug configuration, that is unless you have specificaly built or downloaded a debug version of Python. That said you can make a Debug build if you first modify the contents of PATH_TO_PYTHON_DIST/include/pyconfig.h, changing\npragma comment(lib,\"pythonxx_d.lib\")\nto\npragma comment(lib,\"pythonxx.lib\")\nand\n#       define Py_DEBUG\nto\n//#       define Py_DEBUG\nThen simply follow the instructions above for building with CMake."
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#troubleshooting-python-bindings-installation-issues",
    "href": "accelerate_opencv_cuda_python.html#troubleshooting-python-bindings-installation-issues",
    "title": "Build OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows",
    "section": "Troubleshooting Python Bindings Installation issues",
    "text": "Troubleshooting Python Bindings Installation issues\nIf you are unable to import cv2 without errors then check below to see if there is a solution to the error you recieve.\n\nModuleNotFoundError: No module named 'cv2'\nThe installation of the Python bindings has failed, check\n\nthe build was successful,\n-DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/ was set correctly, and\nif you are still seeing the above error try manually installing opencv Python bindings.\n\n\n\n\n\n\nImportError: ERROR: recursion is detected during loading of \"cv2\" binary extensions. Check OpenCV installation.\nThe main two reasons for this are:\n\nYou have another installation of OpenCV, either manually installed or through the package manager (pip/mamba etc.). This can easily be fixed by first uninstalling any opencv-python, opencv-contrib-python distributions from your package manager and then deleting the cv2 directory (PATH_TO_PYTHON_DIST/Lib/site-packages/cv2/) or bindings file (PATH_TO_PYTHON_DIST/Lib/site-packages/cv2.cpxx-win_amd64.pyd) if they exist.\nYou have built a Debug configuration. Currently (https://github.com/opencv/opencv/issues/23568) when building this configuration the cv2.cpxx-win_amd64.pyd shared library is not copied into site-packages-x.x\ndirectory on installation. This can easily be resolved by creating the python-x.x directory and copying the shared library accross so you have PATH_TO_PYTHON_DIST/Lib/site-packages/cv2/python-x.x/cv2.cpxx-win_amd64.pyd, where xx is the PYTHON_VERSION.\n\nImportError: DLL load failed: The specified procedure could not be found.\nThe directory of one or more of the required DLL’s has not been added with os.add_dll_directory(). Whilst the automatic installation of the bindings should have added all the directories containing the dependant DLL’s to config.py it’s possible that one has been missed or you are using a less common configuration. In these cases you will have to\n\nfirst track down which DLL’s are missing (see this guide for assistance) and then\npermanantly add the directory containing them to your installation by modifying the contents of PATH_TO_PYTHON_DIST/Lib/site-packages/cv2/config.py.\n\ne.g. If you built OpenCV against CUDA 12.1 and your own version of the FFMpeg libraries (-DOPENCV_FFMPEG_USE_FIND_PACKAGE=ON) instead of using the provided opencv_videoio_ffmpegxxx_64.dll plugin, the contents of config.py should look like\nimport os\n\nBINARIES_PATHS = [\n    os.path.join('D:/build/opencv/install', 'x64/vc17/bin'),\n    os.path.join(os.getenv('CUDA_PATH', 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.1'), 'bin')\n    os.path.join(`D:/ffmpeg/bin`)\n] + BINARIES_PATHS"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#manually-installing-opencv-python-bindings",
    "href": "accelerate_opencv_cuda_python.html#manually-installing-opencv-python-bindings",
    "title": "Build OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows",
    "section": "Manually installing OpenCV Python bindings",
    "text": "Manually installing OpenCV Python bindings\nIf you have downloaded the pre-built binaries or are having issues with the automatic installation then you can manually install the python bindings following the steps below:\n\nRemove any pre-existing OpenCV installations.\nCopy PATH_TO_BUILD_DIR/lib/python3/cv2.cpxx-win_amd64.pyd to PATH_TO_PYTHON_DIST/Lib/site-packages/cv2.cpxx-win_amd64.pyd\nDetermine the paths to the directories containing any dependant shared libraries (see here for assistance).\nAdding the locations from (3) by calling os.add_dll_directory() for each one before importing the OpenCV python module. e.g. If you have followed the guide exactly this will be the directories containing the OpenCV and Nvidia shared libaries, which you would add as\nimport os\nos.add_dll_directory('C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\vxx.x\\\\bin')\nos.add_dll_directory('PATH_TO_BUILD_DIR/bin')\nbefore calling\nimport cv2 as cv"
  },
  {
    "objectID": "accelerate_opencv_cuda_python.html#cmake-command-line-options-to-control-cubinptx-content-of-the-opencv-shared-library",
    "href": "accelerate_opencv_cuda_python.html#cmake-command-line-options-to-control-cubinptx-content-of-the-opencv-shared-library",
    "title": "Build OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows",
    "section": "CMake command line options to control cubin/PTX content of the OpenCV shared library",
    "text": "CMake command line options to control cubin/PTX content of the OpenCV shared library\nGiven (1)-(3) above, the command line options that you want to pass to CMake when building OpenCV will depend on your specific requirements. I have given some examples below for various scenarios given a main GPU with compute capability 6.1:\n\nFirstly stick with the defaults if compile time and shared library size are not an issue. This offers the greatest amount of flexibility from a development standpoint, avoiding the possibility of needing to recompile OpenCV when you switch GPU.\nIf your programs will always be run on your main GPU, just pass -DCUDA_ARCH_BIN=6.1 to CMake to target your architecture only. It should take around an hour to build, depending on your CPU and the resulting shared library should not be larger than 200 MB.\nIf you are going to deploy your application, but only to newer GPU’s pass -DCUDA_ARCH_BIN=6.1,7.0,8.0,8.6 and -DCUDA_ARCH_PTX=8.6 to CMake for maximum performance and future compatibility. This is advisable because you may not have any control over the size of the JIT cache on the target machine, therefore including cubin’s for all compute-capabilities you want to support, is the only way be sure to prevent JIT compilation delay on every invocation of your application.\nIf size is really an issue but you don’t know which GPU’s you want to run your application on then to ensure that your program will run on all current and future supported GPU’s pass -DCUDA_ARCH_BIN=6.1 and -DCUDA_ARCH_PTX=3.0 to CMake for maximum coverage."
  },
  {
    "objectID": "how2.html",
    "href": "how2.html",
    "title": "Tutorials",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "howto.html",
    "href": "howto.html",
    "title": "How To lala",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenCV Guides",
    "section": "",
    "text": "ImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBuild OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAccelerating OpenCV with Python and CUDA streams\n\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2019\n\n\n\n\n\n\n  \n\n\n\n\nOpenCV CUDA Performance Comparisson (Nvidia vs Intel)\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "opencv_cuda_performance.html",
    "href": "opencv_cuda_performance.html",
    "title": "OpenCV CUDA Performance Comparisson (Nvidia vs Intel)",
    "section": "",
    "text": "In this post I am going to use the OpenCV’s performance tests to compare the CUDA and CPU implementations. The idea, is to get an indication of which OpenCV and/or Computer Vision algorithms, in general, benefit the most from GPU acceleration, and therefore, under what circumstances it might be a good idea to invest in a GPU."
  },
  {
    "objectID": "opencv_cuda_performance.html#gpu-specifications",
    "href": "opencv_cuda_performance.html#gpu-specifications",
    "title": "OpenCV CUDA Performance Comparisson (Nvidia vs Intel)",
    "section": "GPU specifications",
    "text": "GPU specifications\nThe GPU’s tested comprise three different micro-architectures, ranging from a low end laptop (730m) to a mid range desktop (GTX 1060) GPU. The full specifications are shown below, where I have also included the maximum theoretical speedup, if the OpenCV function were bandwidth or compute limited. This value is just included to give an indication of what should be possible if architectural improvements, SM count etc. don’t have any impact on performance. In “general” most algorithms will be bandwidth limited implying that the average speed up of the OpenCV functions could be somewhere between these two values. If you are not familiar with this concept then I would recommend watching Memory Bandwidth Bootcamp: Best Practices, Memory Bandwidth Bootcamp: Beyond Best Practices and Memory Bandwidth Bootcamp: Collaborative Access Patterns by Tony Scudiero for a good overview."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "OpenCV Guides",
    "section": "",
    "text": "OpenCV CUDA Performance Comparisson (Nvidia vs Intel)\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2018\n\n\n\n\n\n\n  \n\n\n\n\nAccelerating OpenCV with Python and CUDA streams\n\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2019\n\n\n\n\n\n\n  \n\n\n\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBuild OpenCV with CUDA, cuDNN, Nvidia Video Codec SDK and Python on Windows\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials1.html",
    "href": "tutorials1.html",
    "title": "Tutorials",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "tutorials2.html",
    "href": "tutorials2.html",
    "title": "Tutorials",
    "section": "",
    "text": "About this site"
  }
]