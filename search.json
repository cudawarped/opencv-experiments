[
  {
    "objectID": "qmd/opencv_cuda_python_windows.html#building-opencv-with-cuda-using-visual-studio-solution-files-from-the-command-prompt-cmd",
    "href": "qmd/opencv_cuda_python_windows.html#building-opencv-with-cuda-using-visual-studio-solution-files-from-the-command-prompt-cmd",
    "title": "Build OpenCV (including Python) with CUDA on Windows",
    "section": "Building OpenCV with CUDA using Visual Studio solution files from the command prompt (cmd)",
    "text": "Building OpenCV with CUDA using Visual Studio solution files from the command prompt (cmd)\nThe following steps will build opencv_worldxxx.dll which is suitable for use on the installed GPU and any other GPUs with the “same” compute capability. This is the best way to get started as it has the smallest build time, but it will limit your options if you want to build software which runs on a range of different GPUs. To fully understand the implications of this please read choosing a suitable CUDA compute capability.\n\n\n\n\n\n\nUse Ninja for faster build times\n\n\n\n\n\nVisual Studio is painfully slow when compiling OpenCV with CUDA, to reduce the build time I recommended always using the Ninja build system instead, see the Ninja tab below for the command line arguments and building OpenCV with the ninja build system to reduce the build time for more information.\n\n\n\n\nOpen windows command prompt, type cmd in the Windows search bar.\n\n\n\n\n\nPaste the below5 into to the command prompt and press Enter.\n\nAll CUDA modulescuDNN onlyWithout Python bindingsWithout CUDANinja\n\n\nset CMAKE_BUILD_PARALLEL_LEVEL=&lt;N_THREADS&gt;\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -H\"&lt;PATH_TO_OPENCV_SOURCE&gt;\" -DOPENCV_EXTRA_MODULES_PATH=\"&lt;PATH_TO_OPENCV_CONTRIB_MODULES&gt;\" -B\"&lt;PATH_TO_BUILD_DIR&gt;\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DENABLE_CUDA_FIRST_CLASS_LANGUAGE=ON -DWITH_CUDA=ON -DCUDA_GENERATION=Auto -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=&lt;PATH_TO_PYTHON_DIST&gt;/include -DPYTHON3_LIBRARY=&lt;PATH_TO_PYTHON_DIST&gt;/libs/&lt;PYTHON_Lib&gt; -DPYTHON3_EXECUTABLE=&lt;PATH_TO_PYTHON_DIST&gt;/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=&lt;PATH_TO_PYTHON_DIST&gt;/lib/site-packages/numpy/_core/include -DPYTHON3_PACKAGES_PATH=&lt;PATH_TO_PYTHON_DIST&gt;/Lib/site-packages\n\n\nIf you just want to CUDA accelerate the DNN module and are not interested in building the rest of the CUDA modules this will significantly reduce compilation time and size of opencv_worldxxx.dll.\nset CMAKE_BUILD_PARALLEL_LEVEL=&lt;N_THREADS&gt;\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -H\"&lt;PATH_TO_OPENCV_SOURCE&gt;\" -DOPENCV_EXTRA_MODULES_PATH=\"&lt;PATH_TO_OPENCV_CONTRIB_MODULES&gt;\" -B\"&lt;PATH_TO_BUILD_DIR&gt;\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DENABLE_CUDA_FIRST_CLASS_LANGUAGE=ON -DWITH_CUDA=ON -DCUDA_GENERATION=Auto -DBUILD_opencv_cudaarithm=OFF -DBUILD_opencv_cudabgsegm=OFF -DBUILD_opencv_cudafeatures2d=OFF -DBUILD_opencv_cudafilters=OFF -DBUILD_opencv_cudaimgproc=OFF -DBUILD_opencv_cudalegacy=OFF -DBUILD_opencv_cudaobjdetect=OFF -DBUILD_opencv_cudaoptflow=OFF -DBUILD_opencv_cudastereo=OFF -DBUILD_opencv_cudawarping=OFF -DBUILD_opencv_cudacodec=OFF -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=&lt;PATH_TO_PYTHON_DIST&gt;/include -DPYTHON3_LIBRARY=&lt;PATH_TO_PYTHON_DIST&gt;/libs/&lt;PYTHON_Lib&gt; -DPYTHON3_EXECUTABLE=&lt;PATH_TO_PYTHON_DIST&gt;/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=&lt;PATH_TO_PYTHON_DIST&gt;/lib/site-packages/numpy/_core/include -DPYTHON3_PACKAGES_PATH=&lt;PATH_TO_PYTHON_DIST&gt;/Lib/site-packages\n\n\nset CMAKE_BUILD_PARALLEL_LEVEL=&lt;N_THREADS&gt;\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -H\"&lt;PATH_TO_OPENCV_SOURCE&gt;\" -DOPENCV_EXTRA_MODULES_PATH=\"&lt;PATH_TO_OPENCV_CONTRIB_MODULES&gt;\" -B\"&lt;PATH_TO_BUILD_DIR&gt;\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DENABLE_CUDA_FIRST_CLASS_LANGUAGE=ON -DWITH_CUDA=ON -DCUDA_GENERATION=Auto\n\n\nset CMAKE_BUILD_PARALLEL_LEVEL=&lt;N_THREADS&gt;\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -H\"&lt;PATH_TO_OPENCV_SOURCE&gt;\" -DOPENCV_EXTRA_MODULES_PATH=\"&lt;PATH_TO_OPENCV_CONTRIB_MODULES&gt;\" -B\"&lt;PATH_TO_BUILD_DIR&gt;\" -G\"Visual Studio 17 2022\" -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=&lt;PATH_TO_PYTHON_DIST&gt;/include -DPYTHON3_LIBRARY=&lt;PATH_TO_PYTHON_DIST&gt;/libs/&lt;PYTHON_Lib&gt; -DPYTHON3_EXECUTABLE=&lt;PATH_TO_PYTHON_DIST&gt;/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=&lt;PATH_TO_PYTHON_DIST&gt;/lib/site-packages/numpy/_core/include -DPYTHON3_PACKAGES_PATH=&lt;PATH_TO_PYTHON_DIST&gt;/Lib/site-packages\n\n\nFor details see decreasing the build time with Ninja\n\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -H\"&lt;PATH_TO_OPENCV_SOURCE&gt;\" -DOPENCV_EXTRA_MODULES_PATH=\"&lt;PATH_TO_OPENCV_CONTRIB_MODULES&gt;\" -B\"&lt;PATH_TO_BUILD_DIR&gt;\" -G\"Ninja Multi-Config\"\" -DCMAKE_BUILD_TYPE=Release -DINSTALL_TESTS=ON -DINSTALL_C_EXAMPLES=ON -DBUILD_EXAMPLES=ON -DBUILD_opencv_world=ON -DENABLE_CUDA_FIRST_CLASS_LANGUAGE=ON -DWITH_CUDA=ON -DCUDA_GENERATION=Auto -DBUILD_opencv_python3=ON -DPYTHON3_INCLUDE_DIR=&lt;PATH_TO_PYTHON_DIST&gt;/include -DPYTHON3_LIBRARY=&lt;PATH_TO_PYTHON_DIST&gt;/libs/&lt;PYTHON_Lib&gt; -DPYTHON3_EXECUTABLE=&lt;PATH_TO_PYTHON_DIST&gt;/python.exe -DPYTHON3_NUMPY_INCLUDE_DIRS=&lt;PATH_TO_PYTHON_DIST&gt;/lib/site-packages/numpy/_core/include -DPYTHON3_PACKAGES_PATH=&lt;PATH_TO_PYTHON_DIST&gt;/Lib/site-packages\nFollowing the configuration step the build is started with\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target install --config Release\n\n\n\nwhere\n\n&lt;N_THREADS&gt; should be set to the number of cores/logical processors on your CPU,\n&lt;PATH_TO_OPENCV_SOURCE&gt; is the root of the OpenCV files you downloaded or cloned (the directory containing 3rdparty, apps, build, etc.),\n&lt;PATH_TO_OPENCV_CONTRIB_MODULES&gt;6 is the path to the modules directory inside the opencv-contrib repository (the directory containing cudaarithm, cudabgsegm, etc.),\n&lt;PATH_TO_BUILD_DIR&gt; is the path to the directory where the build files should go,\n&lt;PATH_TO_PYTHON_DIST&gt;7 is the directory where miniforge was installed and,\n&lt;PYTHON_LIB&gt; is the concatination of “python” with the major and minor version of your python install and “.lib”, e.g. for Python 3.10.6 &lt;PYTHON_LIB&gt;==python310.lib. You can confirm this by looking in your &lt;PATH_TO_PYTHON_DIST&gt;/libs directory.\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen passing paths to CMake on the command line make sure not to terminate them using \\ as this is a special character and will cause the following arguments to be ignored. e.g. PATH_TO_OPENCV_SOURCE can be D:\\opencv or D:\\opencv/ but not D:\\opencv\\.\n\n\nThis will generate the build files for OpenCV with python bindings with CUDA acceleration including all the corresponding tests and examples for verifcation. Additionally if the Nvidia Video Codec SDK or cuDNN are installed the corresponding modules will automatically be included.\nExpand the tips below for an example of the CMake output if the configuration step is successful and how to check that output to make sure the Python bindings will be being built.\n\n\n\n\n\n\nExample of CMake Configuration Output\n\n\n\n\n\n-- General configuration for OpenCV 4.13.0-dev =====================================\n--   Version control:               4.12.0-63-g3278820f5d\n--\n--   Extra modules:\n--     Location (extra):            D:/repos/opencv/contrib/modules\n--     Version control (extra):     4.12.0-7-g9a9b173c\n--\n--   Platform:\n--     Timestamp:                   2025-08-07T14:26:42Z\n--     Host:                        Windows 10.0.26100 AMD64\n--     CMake:                       4.0.1\n--     CMake generator:             Ninja Multi-Config\n--     CMake build tool:            D:/bin/ninja/ninja.exe\n--     MSVC:                        1944\n--     Configuration:               Debug Release\n--     Algorithm Hint:              ALGO_HINT_ACCURATE\n--\n--   CPU/HW features:\n--     Baseline:                    SSE SSE2 SSE3\n--       requested:                 SSE3\n--     Dispatched code generation:  SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n--       SSE4_1 (19 files):         + SSSE3 SSE4_1\n--       SSE4_2 (2 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n--       AVX (10 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n--       FP16 (1 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 AVX FP16\n--       AVX2 (39 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 AVX FP16 AVX2 FMA3\n--       AVX512_SKX (9 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 AVX FP16 AVX2 FMA3 AVX_512F AVX512_COMMON AVX512_SKX\n--\n--   C/C++:\n--     Built as dynamic libs?:      YES\n--     C++ standard:                11\n--     C++ Compiler:                C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.44.35207/bin/Hostx64/x64/cl.exe  (ver 19.44.35213.0)\n--     C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS    /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819  /O2 /Ob2 /DNDEBUG  /Zi\n--     C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS    /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819  /Zi /Ob0 /Od /RTC1\n--     C Compiler:                  C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.44.35207/bin/Hostx64/x64/cl.exe\n--     C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS      /O2 /Ob2 /DNDEBUG  /Zi\n--     C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise /FS    /Zi /Ob0 /Od /RTC1\n--     Linker flags (Release):      /machine:x64  /INCREMENTAL:NO  /debug\n--     Linker flags (Debug):        /machine:x64  /debug /INCREMENTAL\n--     ccache:                      NO\n--     Precompiled headers:         NO\n--     Extra dependencies:          C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/cudart_static.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppial.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppc.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppitc.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppig.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppist.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppidei.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/cublas.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/cublasLt.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/cufft.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppicc.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppif.lib C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v13.0/lib/x64/nppim.lib\n--     3rdparty dependencies:\n--\n--   OpenCV modules:\n--     To be built:                 aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dnn_superres dpm face features2d flann fuzzy gapi hfs highgui img_hash imgcodecs imgproc intensity_transform line_descriptor mcc ml objdetect optflow phase_unwrapping photo plot python3 quality rapid reg rgbd saliency shape signal stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab wechat_qrcode xfeatures2d ximgproc xobjdetect xphoto\n--     Disabled:                    world\n--     Disabled by dependency:      -\n--     Unavailable:                 alphamat cannops cvv fastcv freetype hdf java julia matlab ovis python2 sfm viz\n--     Applications:                tests perf_tests examples apps\n--     Documentation:               doxygen python\n--     Non-free algorithms:         NO\n--\n--   Windows RT support:            NO\n--\n--   GUI:                           WIN32UI\n--     Win32 UI:                    YES\n--     OpenGL support:              YES (opengl32 glu32)\n--     VTK support:                 NO\n--\n--   Media I/O:\n--     ZLib:                        build (ver 1.3.1)\n--     JPEG:                        build-libjpeg-turbo (ver 3.1.0-70)\n--       SIMD Support Request:      YES\n--       SIMD Support:              NO\n--     WEBP:                        build (ver decoder: 0x0209, encoder: 0x020f, demux: 0x0107)\n--     AVIF:                        NO\n--     PNG:                         build (ver 1.6.43)\n--       SIMD Support Request:      YES\n--       SIMD Support:              YES (Intel SSE)\n--     TIFF:                        build (ver 42 - 4.6.0)\n--     JPEG 2000:                   build (ver 2.5.3)\n--     OpenEXR:                     build (ver 2.3.0)\n--     GIF:                         YES\n--     HDR:                         YES\n--     SUNRASTER:                   YES\n--     PXM:                         YES\n--     PFM:                         YES\n--\n--   Video I/O:\n--     FFMPEG:                      YES (find_package)\n--       avcodec:                   YES (59.18.100)\n--       avformat:                  YES (59.16.100)\n--       avutil:                    YES (57.17.100)\n--       swscale:                   YES (6.4.100)\n--       avresample:                NO\n--     GStreamer:                   NO\n--     DirectShow:                  YES\n--     Media Foundation:            YES\n--       DXVA:                      YES\n--\n--   Parallel framework:            Concurrency\n--\n--   Trace:                         YES (with Intel ITT(3.25.4))\n--\n--   Other third-party libraries:\n--     Intel IPP:                   2022.1.0 [2022.1.0]\n--            at:                   D:/build/opencv/cuda_13/3rdparty/ippicv/ippicv_win/icv\n--     Intel IPP IW:                sources (2022.1.0)\n--               at:                D:/build/opencv/cuda_13/3rdparty/ippicv/ippicv_win/iw\n--     Lapack:                      NO\n--     Eigen:                       NO\n--     Custom HAL:                  YES (ipp (ver 0.0.1))\n--     Protobuf:                    build (3.19.1)\n--     Flatbuffers:                 builtin/3rdparty (23.5.9)\n--\n--   NVIDIA CUDA:                   YES (ver 13.0.48, CUFFT CUBLAS NVCUVID NVCUVENC)\n--     NVIDIA GPU arch:             86\n--     NVIDIA PTX archs:\n--\n--   cuDNN:                         YES (ver 9.11.0)\n--\n--   OpenCL:                        YES (NVD3D11)\n--     Include path:                D:/repos/opencv/opencv/3rdparty/include/opencl/1.2\n--     Link libraries:              Dynamic load\n--\n--   Python 3:\n--     Interpreter:                 C:/Users/b/miniforge3/python.exe (ver 3.10.14)\n--     Libraries:                   C:/Users/b/miniforge3/libs/python310.lib (ver 3.10.14)\n--     Limited API:                 NO\n--     numpy:                       C:/Users/b/miniforge3/lib/site-packages/numpy/_core/include (ver 2.2.6)\n--     install path:                C:/Users/b/miniforge3/Lib/site-packages/cv2/python-3.10\n--\n--   Python (for build):            C:/Users/b/miniforge3/python.exe\n--\n--   Java:\n--     ant:                         NO\n--     Java:                        NO\n--     JNI:                         NO\n--     Java wrappers:               NO\n--     Java tests:                  NO\n--\n--   Install to:                    D:/build/opencv/cuda_13/install\n-- -----------------------------------------------------------------\n--\n-- Configuring done (28.9s)\n-- Generating done (5.9s)\n-- Build files have been written to: D:/build/opencv/cuda_13\n\n\n\n\n\n\n\n\n\nVerify configuration includes Python bindings before building\n\n\n\n\n\nIf you are building the python bindings look for python3 in the To be built: section of your CMake configuration output and if its not present look for any python related errors in the output preceeding it. e.g.\n--   OpenCV modules:\n--     To be built:                 aruco bgsegm bioinspired calib3d ccalib core cudaarithm cudabgsegm cudacodec cudafeatures2d cudafilters cudaimgproc cudalegacy cudaobjdetect cudaoptflow cudastereo cudawarping cudev datasets dnn dnn_objdetect dpm face features2d flann fuzzy hfs highgui img_hash imgcodecs imgproc line_descriptor ml objdetect optflow phase_unwrapping photo plot python2 python3 quality reg rgbd saliency shape stereo stitching structured_light superres surface_matching text tracking ts video videoio videostab world xfeatures2d ximgproc xobjdetect xphoto\n\n\n\nThe OpenCV.sln solution file should now be in your PATH_TO_BUILD_DIR directory. To build OpenCV you have two options depending on you preference you can:\n\nBuild directly from the command line by simply entering the following (swaping Release for Debug to build a release version)\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target install --config Debug\nBuild through Visual Studio GUI by opening up the OpenCV.sln in Visual Studio, selecting your Configuration, clicking on Solution Explorer, expanding CMakeTargets, right clicking on INSTALL and clicking Build.\n\n\n\nVisual Studio Build Solution\n\n\n\nEither approach will both build the library, install the Python bindings and copy the necessary redistributable parts to the install directory (PATH_TO_BUILD_DIR/build/install). All that is required now to run any programs compiled against these libs is to add the directory containing opencv_worldxxx.dll to you user path environmental variable.\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default you have to build Release when generating python bindings, for instructions on how to build Debug see generate python bindings for a debug build\n\n\nIf everything was successful, congratulations, you now have OpenCV built with CUDA. To quickly verify that the CUDA modules are working and check if there is any performance benefit on your specific hardware see verifying OpenCV is CUDA accelerated.",
    "crumbs": [
      "OpenCV",
      "Build OpenCV (including Python) with CUDA on Windows"
    ]
  },
  {
    "objectID": "qmd/opencv_cuda_python_windows.html#decreasing-the-build-time-with-ninja",
    "href": "qmd/opencv_cuda_python_windows.html#decreasing-the-build-time-with-ninja",
    "title": "Build OpenCV (including Python) with CUDA on Windows",
    "section": "Decreasing the build time with Ninja",
    "text": "Decreasing the build time with Ninja\nThe build time for OpenCV can be reduced by more than 2x (from 2 hours to 30 mins to under an hour on an i7-8700) by utilizing the Ninja build system instead of directly generating Visual Studio solution files.\nNinja is installed by default if the Desktop development with C++ workload is selected when installing Visual Studio, therefore building with Ninja only requires two extra configuration steps:\n\nConfiguring Visual Studio Development tools by entering the following into the command prompt before entering the CMake command (changing Community to either Professional or Enterprise if necessary)\n\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\nTelling CMake to use Ninja instead of Visual Studio, i.e. replacing -G\"Visual Studio 17 2022\" with -G\"Ninja Multi-Config\".\n\n\nOnce the build files have been generated the build can be kicked off in the same way as before. i.e.\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" --build PATH_TO_BUILD_DIR --target install --config Release\nfor an example fo the full command line for building a Release version of OpenCV with the Ninja build system go to the Ninja tab.",
    "crumbs": [
      "OpenCV",
      "Build OpenCV (including Python) with CUDA on Windows"
    ]
  },
  {
    "objectID": "qmd/opencv_cuda_python_windows.html#generate-python-bindings-for-a-debug-build",
    "href": "qmd/opencv_cuda_python_windows.html#generate-python-bindings-for-a-debug-build",
    "title": "Build OpenCV (including Python) with CUDA on Windows",
    "section": "Generate Python bindings for a Debug Build",
    "text": "Generate Python bindings for a Debug Build\nPython bindings cannot by default be generated for a Debug configuration, that is unless you have specificaly built or downloaded a debug version of Python. That said you can make a Debug build if you first modify the contents of PATH_TO_PYTHON_DIST/include/pyconfig.h, changing\npragma comment(lib,\"pythonxx_d.lib\")\nto\npragma comment(lib,\"pythonxx.lib\")\nand\n#       define Py_DEBUG\nto\n//#       define Py_DEBUG\nThen simply follow the instructions above for building with CMake.",
    "crumbs": [
      "OpenCV",
      "Build OpenCV (including Python) with CUDA on Windows"
    ]
  },
  {
    "objectID": "qmd/opencv_cuda_python_windows.html#troubleshooting-python-bindings-installation-issues",
    "href": "qmd/opencv_cuda_python_windows.html#troubleshooting-python-bindings-installation-issues",
    "title": "Build OpenCV (including Python) with CUDA on Windows",
    "section": "Troubleshooting Python Bindings Installation issues",
    "text": "Troubleshooting Python Bindings Installation issues\nIf you are unable to import cv2 without errors then check below to see if there is a solution to the error you recieve.\n\nModuleNotFoundError: No module named 'cv2'\nThe installation of the Python bindings has failed, check\n\nthe build was successful,\n-DPYTHON3_PACKAGES_PATH=PATH_TO_PYTHON_DIST/Lib/site-packages/ was set correctly, and\nif you are still seeing the above error try manually installing opencv Python bindings.\n\n\n\n\n\n\nImportError: ERROR: recursion is detected during loading of \"cv2\" binary extensions. Check OpenCV installation.\nThe main two reasons for this are:\n\nYou have another installation of OpenCV, either manually installed or through the package manager (pip/mamba etc.). This can easily be fixed by first uninstalling any opencv-python, opencv-contrib-python distributions from your package manager and then deleting the cv2 directory (PATH_TO_PYTHON_DIST/Lib/site-packages/cv2/) or bindings file (PATH_TO_PYTHON_DIST/Lib/site-packages/cv2.cpxx-win_amd64.pyd) if they exist.\nYou have built a Debug configuration. Currently (https://github.com/opencv/opencv/issues/23568) when building this configuration the cv2.cpxx-win_amd64.pyd shared library is not copied into site-packages-x.x\ndirectory on installation. This can easily be resolved by creating the python-x.x directory and copying the shared library accross so you have PATH_TO_PYTHON_DIST/Lib/site-packages/cv2/python-x.x/cv2.cpxx-win_amd64.pyd, where xx is the PYTHON_VERSION.\n\nImportError: DLL load failed: The specified module could not be found.\nThe directory of one or more of the required DLL’s has not been added with os.add_dll_directory(). Whilst the automatic installation of the bindings should have added all the directories containing the dependant DLL’s to config.py it’s possible that one has been missed or you are using a less common configuration. In these cases you will have to\n\nfirst track down which DLL’s are missing (see this guide for assistance) and then\npermanantly add the directory containing them to your installation by modifying the contents of PATH_TO_PYTHON_DIST/Lib/site-packages/cv2/config.py.\n\ne.g. If you built OpenCV against CUDA 12.1 and your own version of the FFMpeg libraries (-DOPENCV_FFMPEG_USE_FIND_PACKAGE=ON) instead of using the provided opencv_videoio_ffmpegxxx_64.dll plugin, the contents of config.py should look like\nimport os\n\nBINARIES_PATHS = [\n    os.path.join('D:/build/opencv/install', 'x64/vc17/bin'),\n    os.path.join(os.getenv('CUDA_PATH', 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.5'), 'bin')\n    os.path.join(`D:/ffmpeg/bin`)\n] + BINARIES_PATHS\nImportError: DLL load failed while importing cv2: A dynamic link library (DLL) initialization routine failed.\nThe most common cause of this is out of date Visual C++ Redistributable libraries. These can be loaded from your windows system or your python installation depending on your configuration. The easiest thing to do is update both by\n\ndownloading and installing the latest version of the Visual C++ Redistributable from Microsoft, and\nchecking your python package manager (pip/mamba/conda/… list) for the vc or vc14_runtime packages and updating them to the latest version (e.g. pip install vc --upgrade or mamba/conda upgrade vc) and try importing OpenCV again.",
    "crumbs": [
      "OpenCV",
      "Build OpenCV (including Python) with CUDA on Windows"
    ]
  },
  {
    "objectID": "qmd/opencv_cuda_python_windows.html#manually-installing-opencv-python-bindings",
    "href": "qmd/opencv_cuda_python_windows.html#manually-installing-opencv-python-bindings",
    "title": "Build OpenCV (including Python) with CUDA on Windows",
    "section": "Manually installing OpenCV Python bindings",
    "text": "Manually installing OpenCV Python bindings\nIf you have downloaded the pre-built binaries or are having issues with the automatic installation then you can manually install the python bindings following the steps below:\n\nRemove any pre-existing OpenCV installations.\nCopy PATH_TO_BUILD_DIR/lib/python3/cv2.cpxx-win_amd64.pyd to PATH_TO_PYTHON_DIST/Lib/site-packages/cv2.cpxx-win_amd64.pyd\nDetermine the paths to the directories containing any dependant shared libraries (see here for assistance).\nAdding the locations from (3) by calling os.add_dll_directory() for each one before importing the OpenCV python module. e.g. If you have followed the guide exactly this will be the directories containing the OpenCV and Nvidia shared libaries, which you would add as\nimport os\nos.add_dll_directory('C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\vxx.x\\\\bin')\nos.add_dll_directory('PATH_TO_BUILD_DIR/bin')\nbefore calling\nimport cv2 as cv",
    "crumbs": [
      "OpenCV",
      "Build OpenCV (including Python) with CUDA on Windows"
    ]
  },
  {
    "objectID": "qmd/opencv_cuda_python_windows.html#cmake-command-line-options-to-control-cubinptx-content-of-the-opencv-shared-library",
    "href": "qmd/opencv_cuda_python_windows.html#cmake-command-line-options-to-control-cubinptx-content-of-the-opencv-shared-library",
    "title": "Build OpenCV (including Python) with CUDA on Windows",
    "section": "CMake command line options to control cubin/PTX content of the OpenCV shared library",
    "text": "CMake command line options to control cubin/PTX content of the OpenCV shared library\nGiven (1)-(3) above, the command line options that you want to pass to CMake when building OpenCV will depend on your specific requirements. I have given some examples below for various scenarios given a main GPU with compute capability 6.1 and CUDA toolkit 12:\n\nFirstly stick with the defaults (-DCUDA_GENERATION=Auto) if your programs will always be run on your main GPU. It should take around an hour to build, depending on your CPU and the resulting shared library should not be larger than 200 MB.\nIf you want maximum coverage then use Nvidia’s recommended settings for future compatibility. That is compile for all supported major and minor real architectures, and the highest major virtual architecture by passing -DCUDA_ARCH_BIN=50,52,60,61,70,75,80,86,89,90 and -DCUDA_ARCH_PTX=90 to CMake.\nIf you are going to deploy your application, but only to newer GPU’s pass -DCUDA_ARCH_BIN=61,70,80,86,89,90 and -DCUDA_ARCH_PTX=90 to CMake for maximum performance and future compatibility. This is advisable because you may not have any control over the size of the JIT cache on the target machine, therefore including cubin’s for all compute-capabilities you want to support, is the only way be sure to prevent JIT compilation delay on every invocation of your application.\nIf size is really an issue but you don’t know which GPU’s you want to run your application on then to ensure that your program will run on all current and future supported GPU’s pass -DCUDA_ARCH_BIN=61 and -DCUDA_ARCH_PTX=30 to CMake for maximum coverage.",
    "crumbs": [
      "OpenCV",
      "Build OpenCV (including Python) with CUDA on Windows"
    ]
  },
  {
    "objectID": "qmd/opencv_cuda_python_windows.html#footnotes",
    "href": "qmd/opencv_cuda_python_windows.html#footnotes",
    "title": "Build OpenCV (including Python) with CUDA on Windows",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have seen lots of guides including instructions to download and use git to get the source files, however this is a completely unnecessary step. If you are a developer and you don’t already have git installed and configured then I would assume there is a good reason for this and I would not advise installing it just to build OpenCV.↩︎\nBefore building you may want to ensure that your GPU has decoding support by referring to Nvidia Video Decoder Support Matrix↩︎\nAll python distributions should work however I recommend building and testing against a new install of this compact distribution to avoid any problems caused by existing configurations.↩︎\nIf you have any build issues with OpenCV then you will need to provide the arguments passed to CMake as well as the output from the generation step when asking for assistance which is another good reason to avoid the CMake GUI if you can.↩︎\nAn additionally option you may want to include is -DCUDA_FAST_MATH=ON which compiles the CUDA kernels with the -use_fast_math option. This will however cause some of the accuracy and performance tests to fail as the floating point results will be slightly less accurate.↩︎\nIf you get the following error “CUDA : OpenCV requires enabled ‘cudev’ module from ‘opencv_contrib’” when configuring the build with CMake you have not set OPENCV_EXTRA_MODULES_PATH correctly, most likely you have set it to the root of the opencv_contrib repo and not the modules directory inside the repo.↩︎\nThe default installation directory for miniforge is %userprofile%\\miniforge3.↩︎\nThe GEMM test is used in this example but any of the accuracy (opencv_test_cuda*.exe) or performance (opencv_perf_cuda*.exe) tests could have been chosen.↩︎",
    "crumbs": [
      "OpenCV",
      "Build OpenCV (including Python) with CUDA on Windows"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenCV Guides",
    "section": "",
    "text": "ImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\nDefinitive guide to fixing the ‘ImportError: DLL load failed while importing cv2: The specified module could not be found.’ error when trying to import the OpenCV python module.\n\n\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBuild OpenCV (including Python) with CUDA on Windows\n\n\nGuide to building OpenCV (including Python bindings) with CUDA (optionally the Nvidia Video Codec SDK and cuDNN) from within Visual Studio or from the command line using the Ninja build system.\n\n\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAccelerating OpenCV with Python and CUDA streams\n\n\nOpenCV CUDA optimization example using Python and CUDA streams. Including GPU profiling, analysis, performance tips and more!\n\n\n\n\n\nOct 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nOpenCV CUDA Performance Comparison (Nvidia vs Intel)\n\n\n\n\n\n\n\n\nFeb 28, 2018\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "OpenCV",
      "OpenCV Guides"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html",
    "href": "nbs/opencv_cuda_streams_performance_python.html",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "",
    "text": "Since August 2018 the OpenCV CUDA API has been exposed to python. To get the most from this new functionality you need to have a basic understanding of CUDA (most importantly that it is data not task parallel) and its interaction with OpenCV. Below I have tried to introduce these topics with an example of how you could optimize a toy video processing pipeline. The actual functions called in the pipeline are not important, they are simply there to simulate a common processing pipeline consisting of work performed on both the host (CPU) and device (GPU).\nThis guide is taken from a Jupyter Notebook which can be cloned from here. The procedure is as follows, following some quick initialization, we start with a base implementation on both the CPU and GPU to get a baseline result. We then proceed to incrementally improve the implementation by using the information provided by the Nvidia Visual Profiler.\nOn a laptop RTX 2080 paired with an i7-8700 the final CUDA incarnation resulted in a speed up of ~30x and ~10x over the naive CPU and GPU implementations.",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#cpu",
    "href": "nbs/opencv_cuda_streams_performance_python.html#cpu",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "CPU",
    "text": "CPU\n\n\nExpand to inspect the source code\nbgmog2 = cv.createBackgroundSubtractorMOG2()\ndef ProcFrameCPU0(frame,lr,store_res=False):\n    frame_big = cv.resize(frame,(cols_big,rows_big))\n    fg_big = bgmog2.apply(frame_big,learningRate = lr)\n    fg_small = cv.resize(fg_big,(frame.shape[1],frame.shape[0]))\n    if(store_res):\n        cpu_res.append(np.copy(fg_small))\ncpu_res = []\ncpu_time_0, n_frames = ProcVid0(partial(ProcFrameCPU0,store_res=check_res),lr)\nprint(f'CPU 0 (naive): {n_frames} frames, {cpu_time_0:.2f} ms/frame')\n\n\nCPU 0 (naive): 100 frames, 30.05 ms/frame",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#gpu",
    "href": "nbs/opencv_cuda_streams_performance_python.html#gpu",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "GPU",
    "text": "GPU\n\n\nExpand to inspect the source code\nbgmog2_device = cv.cuda.createBackgroundSubtractorMOG2()\ndef ProcFrameCuda0(frame,lr,store_res=False):\n    frame_device.upload(frame)\n    frame_device_big = cv.cuda.resize(frame_device,(cols_big,rows_big))\n    fg_device_big = bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null())\n    fg_device = cv.cuda.resize(fg_device_big,frame_device.size())\n    fg_host = fg_device.download()\n    if(store_res):\n        gpu_res.append(np.copy(fg_host))\ngpu_res = []\ngpu_time_0, n_frames = ProcVid0(partial(ProcFrameCuda0,store_res=check_res),lr)\nprint(f'GPU 0 (naive): {n_frames} frames, {gpu_time_0:.2f} ms/frame')\nprint(f'Speedup over CPU: {cpu_time_0/gpu_time_0:.2f}')\n\n\nGPU 0 (naive): 100 frames, 3.92 ms/frame\nSpeedup over CPU: 7.67\n\n\n\n\nAnalysis\n\n\n\n\nBase Implementation\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe output gpu_time_0 from above is the average amount of time to process each frame, recorded on the host. This will be referred to as the frame time and is the value that we want to reduce. In order to achieve this we need to investigate what is actually occurring on the host and device for each frame. Luckily the Nvidia provides a useful visual tool for this, the Nvidia Visual Profiler.\nThe image above shows the Nvidia Visual Profiler output from processing 2 of the 100 frames. Important things to be aware of here are:\n\nThe runtime API calls in brown which in this example represent the time the host (CPU) spends waiting for the device (GPU) calls to return.\nThe remaining blocks which show the time spent on the device. This is split according to the operation (kernel, memset, MemCpy(HtoD), MemCpy(DtoH)) as well as by the CUDA stream which the operations are issued to. In this case everything is issued to the Default stream.\nThe 0.93ms gap in between the blocks of runtime API calls represents the time spent executing code on the host, here that is the time taken for OpenCV to read and decode each video frame, frame = cap.read().\nIn this naive implementation all device calls from the host are synchronous and as a result the difference between (1) and (2) can be interpreted as periods where no useful work is being performed on either the host or the device. The host is blocking waiting for the device to return and the device is also idle, allocating or freeing memory.\n\nFrom now on for convenience, for a single frame, I will refer to 1), 2) and 3) as the runtime API time, device time, host time respectively. As shown the profiler output, the current runtime API time and host time are ~2.38ms and ~0.93ms.\nTaking (1) and (4) into account from left to right the output from the profiler can be mapped to the python calls as:\n\n(1217.62ms-1220ms) proc_frame_func(frame,lr): calls to the device to process the first frame (~2.38ms)\n(1220ms-1220.93ms) frame = cap.read(): read and decode the second video frame on the host (~0.93ms)\n(1220.93ms-) proc_frame_func(frame,lt): calls to the device to process the second frame\n\nClearly from the gaps described in (4) a lot of time is wasted waiting for the device calls to return, and as the host time does not overlap the device time, there is a lot of room for improvement.\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nThe main causes of (4) are the blocking calls to both\n\ncudaMallocPitch() - OpenCV in python automatically allocates any arrays (NumPy or GpuMat) which are returned from a function call. That is on every iteration\nret, frame = cap.read()\ncauses memory for the NumPy array frame to be allocated and destroyed on the host and\nframe_device_big = cv.cuda.resize(frame_device,(cols_big,rows_big))\nfg_device_big = bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null())\nfg_device = cv.cuda.resize(fg_device_big,frame_device.size())\ncauses memory for frame_device_big, fg_device_big and fg_device to be allocated and destroyed on the device.\ncudaDeviceSynchronise() - if you don’t explicitly pass in a CUDA stream to an OpenCV CUDA function, the default stream will be used and cudaDeviceSynchronize() will be called before the function exits, stalling the GPU every time.\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nAddress the unnecessary calls to cudaMallocPitch(), by pre-allocating any output arrays and passing them as input arguments.",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#cpu-1",
    "href": "nbs/opencv_cuda_streams_performance_python.html#cpu-1",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "CPU",
    "text": "CPU\n\n\nExpand to inspect the source code\nclass ProcFrameCpu1:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.createBackgroundSubtractorMOG2()\n        self.frame = np.empty((rows_small,cols_small,3),np.uint8)\n        self.frame_big = np.empty((rows_big,cols_big,3),np.uint8)\n        self.fg_big = np.empty((rows_big,cols_big),np.uint8)\n        self.fg_small = np.empty((rows_small,cols_small),np.uint8)\n        \n    def ProcessFrame(self,lr):\n        cv.resize(self.frame,(self.cols_big,self.rows_big),self.frame_big)\n        self.bgmog2.apply(self.frame_big,self.fg_big,learningRate = lr)\n        cv.resize(self.fg_big,(self.cols_small,self.rows_small),self.fg_small)\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_small))\n        \n    def Frame(self):\n        return self.frame\n    \ncap = cv.VideoCapture(vidPath)\nif (cap.isOpened()== False): \n  print(\"Error opening video stream or file\")\nret, frame = cap.read()\ncap.release()\nrows_small,cols_small = frame.shape[:2]\nproc_frame_cpu1 = ProcFrameCpu1(rows_small,cols_small,rows_big,cols_big,check_res)\ncpu_time_1, n_frames = ProcVid1(proc_frame_cpu1,lr)\nprint(f'CPU 1 (pre-allocation): {n_frames} frames, {cpu_time_1:.2f} ms/frame')\nprint(f'Speedup over CPU baseline: {cpu_time_0/cpu_time_1:.2f}')\n\n\nCPU 1 (pre-allocation): 100 frames, 27.76 ms/frame\nSpeedup over CPU baseline: 1.08",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#gpu-1",
    "href": "nbs/opencv_cuda_streams_performance_python.html#gpu-1",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "GPU",
    "text": "GPU\n\n\nExpand to inspect the source code\nclass ProcFrameCuda1:\n    def __init__(self,rows_small,cols_small,rows_big,cols_big,store_res=False):\n        self.rows_small, self.cols_small, self.rows_big, self.cols_big = rows_small,cols_small,rows_big,cols_big\n        self.store_res = store_res\n        self.res = []\n        self.bgmog2 = cv.cuda.createBackgroundSubtractorMOG2()\n        self.frame = np.empty((rows_small,cols_small,3),np.uint8)\n        self.frame_device = cv.cuda_GpuMat(rows_small,cols_small,cv.CV_8UC3)\n        self.frame_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC3)        \n        self.fg_device_big = cv.cuda_GpuMat(rows_big,cols_big,cv.CV_8UC1)\n        self.fg_device_big.setTo(0)\n        self.fg_device = cv.cuda_GpuMat(np.shape(frame)[0],np.shape(frame)[1],cv.CV_8UC1)\n        self.fg_host = np.empty((rows_small,cols_small),np.uint8)\n        \n    def ProcessFrame(self,lr):\n        self.frame_device.upload(self.frame)\n        cv.cuda.resize(self.frame_device,(cols_big,rows_big),self.frame_device_big)\n        self.bgmog2.apply(self.frame_device_big,lr,cv.cuda.Stream_Null(),self.fg_device_big)\n        cv.cuda.resize(self.fg_device_big,self.fg_device.size(),self.fg_device)\n        self.fg_device.download(self.fg_host)\n        if(self.store_res):\n            self.res.append(np.copy(self.fg_host))\n        \n    def Frame(self):\n        return self.frame\n    \nproc_frame_cuda1 = ProcFrameCuda1(rows_small,cols_small,rows_big,cols_big,check_res)\ngpu_time_1, n_frames = ProcVid1(proc_frame_cuda1,lr)\nprint(f'GPU 1 (pre-allocation): {n_frames} frames, {gpu_time_1:.2f} ms/frame')\nprint(f'Incremental speedup: {gpu_time_0/gpu_time_1:.2f}')\nprint(f'Speedup over CPU: {cpu_time_1/gpu_time_1:.2f}')\n\n\nGPU 1 (pre-allocation): 100 frames, 1.99 ms/frame\nIncremental speedup: 1.96\nSpeedup over CPU: 13.91",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#analysis-1",
    "href": "nbs/opencv_cuda_streams_performance_python.html#analysis-1",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nPre-allocation of return arrays\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nPre-allocating the arrays has successfully removed the calls to cudaMallocPitch() and significantly (3 frames are now processed instead of 1.5) reduced (4), the time the host spends waiting for the CUDA runtime to return control to it.\nPre-allocation on the host has also reduced the host time from ~0.93ms to ~0.57ms. The host time will now be unaffected by the remaining changes we make and can be observed to be approximately constant after each of the following optimizations.\nWe will now proceed to try and reduce the runtime API time which in this step has already been fallen from ~2.38ms to ~1.15ms.\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nAs mentioned above by not specifying a CUDA stream all calls are placed in the “Default” stream which can be seen at the bottom of the figure. This means that due to the way OpenCV is implemented following each asynchronous kernel launch there will be an internal synchronizing call to cudaDeviceSynchronize() shown below:\n1cv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big)\ncudaDeviceSynchronize()\n2bgmog2_device.apply(frame_device_big,lr,cv.cuda.Stream_Null(),fg_device_big)\ncudaDeviceSynchronize()\n3cv.cuda.resize(fg_device_big,fg_device.size(),fg_device)\ncudaDeviceSynchronize()\n4fg_device.download(fg_host)\n\n1\n\nasynchronous kernel 1\n\n2\n\nasynchronous kernel 2\n\n3\n\nasynchronous kernel 3\n\n4\n\nsynchronous copy from device to host\n\n\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nPass a non default CUDA stream to each OpenCV CUDA function.",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#analysis-2",
    "href": "nbs/opencv_cuda_streams_performance_python.html#analysis-2",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nReplacing the default stream\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe calls to cudaDeviceSyncronize() have now been removed, and as a result the gaps between the device calls have disappeared, further reducing the runtime API time from ~1.15ms to ~1.07ms. That said it looks like the calls to cudaDeviceSyncronize() have just been replaced by calls to cudaMemcpy2DAsync().\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nWhat has actually happened is we have tried to use asynchronous copies to and from the device without first pinning the host memory. Therefore what is shown are three asynchronous kernel launches and a synchronous copy from the device to the host, which blocks the host thread until all the previous work on the device is complete:\n    cv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big,stream=stream) async kernel 1\n    bgmog2.apply(frame_device_big,lr,stream,fg_device_big) acync kernel 2\n    cv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3\n    fg_device.download(stream,fg_host) synchronous copy\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nPin the host memory.",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#analysis-3",
    "href": "nbs/opencv_cuda_streams_performance_python.html#analysis-3",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation (1)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe output is now more intuitive, that said all that we have done is replace the calls to cudaDeviceSyncronize() with calls to cudaStreamSyncronize().\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nWe are issuing asynchronous calls to the device and then immediately waiting on the host for them to complete.\ncv.cuda.resize(frame_device,(cols_big,rows_big),frame_device_big,stream=stream) async kernel 1\nbgmog2.apply(frame_device_big,lr,stream,fg_device_big) async kernel 2\ncv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3\nfg_device.download(stream,fg_host.array) async copy DtoH\nstream.waitForCompletion() block until kernel 1-3 and copy have finished\nWhat we really want to do is overlap host and device computation by issuing asynchronous calls to the device and then performing processing on the host, before waiting for the asynchronous device calls to return. For two frames this would be:\nframe_device.upload(frame[0].array,stream) async copy HtoD, frame 0\ncv.cuda.resize(frame_device,(n_cols_big,n_rows_big),frame_device_big,stream=stream) async kernel 1, frame 0\nbgmog2.apply(frame_device_big,lr,stream,fg_device_big) async kernel 2, frame 0\ncv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3, frame 0\nfg_device.download(stream,fg_host.array) async copy DtoH, frame 0\nret,_ = cap.read(frame[1].array) host read frame 1\nstream.waitForCompletion() block until kernel 1-3 and copy have finished for frame 0\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nMove the position of the synchronization point to after a new frame has been read as described above. To do this we also need to increase the number of host frame containers to two because moving the sync point means frame 0 may still be in the process of being uploaded to the device when we read frame 1. That is, when we call\nret,_ = cap.read(frame[1].array)\nwe have not synced, and we have no way to know if the previous call to\nframe_device.upload(frame[0].array,stream)\nhas finished, hence we need to read to frame[1].array and not frame[0].array.",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#analysis-4",
    "href": "nbs/opencv_cuda_streams_performance_python.html#analysis-4",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation (2)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nAt first glance changing the synchronization point does not appear to have done anything the cudaStreamSynchronize() (stream.waitForCompletion()) still starts at the point just before the frame is processed on the device. On closer inspection we can see that the runtime API time (~1.5ms) now begins much earlier than the device time (~0.8ms) and as we intended overlaps the host time. That said we are not seeing any host/device processing overlap, so whats going on?\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nThis is most likely to be because we are working on Windows where the GPU is a Windows Display Driver Model device. See below for more details.\n\nCUDA driver has a software queue for WDDM devices to reduce the average overhead of submitting command buffers to the WDDM KMD driver\n\nThis would cause all the device calls from the previous frame to be queued and then issued when we call stream.waitForCompletion() and could explain the profiler output.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nTest the hypothesis by forcing the CUDA driver to dispatch all queued calls by issuing a call to stream.queryIfComplete() as shown below.\nframe_device.upload(frames_in[0].array, stream) async copy HtoD, frame 0\ncv.cuda.resize(frame_device,(n_cols_big,n_rows_big),frame_device_big,stream=stream) async kernel 1, frame 0\nbgmog2.apply(frame_device_big, lr, stream, fg_device_big ) async kernel 2, frame 0\ncv.cuda.resize(fg_device_big,fg_device.size(),fg_device,stream=stream) acync kernel 3, frame 0\nfg_device.download(stream,fg_host.array) async copy DtoH, frame 0\nstream.queryIfComplete() force WDDM to dispatch any queued device calls\nret,_ = cap.read(frame[1].array) host read frame 1\nstream.waitForCompletion() block until kernel 1-3 and copy have finished for frame 0",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#analysis-5",
    "href": "nbs/opencv_cuda_streams_performance_python.html#analysis-5",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation (3)\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nIt appears as though the WDDM driver was at fault, by including the extra call to stream.queryIfComplete() we have finally overlapped the processing on the host and device. This can be observed in the profiler output where the host time (~0.62ms), overlaps the device time (~0.79ms) in Stream 2017. Notice also that there are gaps between the blocks of device time in Stream 2017 with the runtime API time (~1.07ms) still starting sometime before the device time and ending exactly after the Memcpy (DtoH) (fg_device.download(stream,fg_host.array)).\nMost importantly the device is almost saturated with only the small gap (~0.2ms) in between each block representing the device time for each frame in Stream 2017. So what is causing this small gap?\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nThe device is stalling.\nAs already mentioned the host time cannot be changed. Additionally from the profiler output it is clear that the host time (~0.62ms) is less than the device time (~0.79ms).\nThat is given the processing pipeline below;\n\nprocess frame 0 on the device ~0.79ms (copy frame 0 to the device execute kernel 1-3 and copy back to the host)\nret,_ = cap.read(frame[1].array) ~0.62ms (read frame 1 on the host)\nstream.waitForCompletion() block for (~0.17ms = 0.79ms-0.62ms) until processing for frame 0 has finished\n\nstream.waitForCompletion() will on average cause the host to wait ~0.17ms for the device processing to finish. This can be observed in the profiler output by the length of cudaStreamchronize() which for each frame ends exactly following the Memcpy(DtoH). Unfortunately this wait stalls the device because it has no work to perform until more calls are issued by the host, which in this case does not occur until after the call to stream.waitForCompletion(). If only there was a way to issue work to the device in advance of stream.waitForCompletion(), which will continue to be performed afterwards.\nFortunately there is by using multiple streams, each processing a single frame at a time. This allows us to issue commands in advance, to process frame 1 before we start the wait on the host for frame 0, shown below\n\nret,_ = cap.read(frame[0].array) host read frame 0\nProcess frame 0 in stream 0\nret,_ = cap.read(frame[1].array) host read frame 1\nProcess frame 1 in stream 1\nret,_ = cap.read(frame[2].array) host read frame 2\nstream[0].waitForCompletion() block until (2) the processing for frame 0 has finished, allowing the device to continue with (4)\nProcess frame 2 in stream 0\nret,_ = cap.read(frame[0].array) host read frame 3\nstream[1].waitForCompletion() block until (4) the processing for frame 1 has finished, allowing the device to continue with (7)\n…\n\nNotice that when stream[0].waitForCompletion() is called the device has Process frame 1 in stream 1 already queued up in stream 1 meaning that the wait on the host should not cause a stall on the device.\nNote: Using multiple streams in this way will add additional latency and is not going to be suitable for real time processing, that said the additional latency in most real world cases will be tolerable and worth the reduction in processing time.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\nUse multiple streams.",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#analysis-6",
    "href": "nbs/opencv_cuda_streams_performance_python.html#analysis-6",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\nOverlap host and device computation - multiple streams\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe device is now completely saturated with memory operations overlapping kernel executions in Streams 2418 and 2419. Additionally the host and device time completely overlap each other. By saturating the device and overlapping host and device computation we have probably reached the limit of the optimizations we can apply to this particular toy problem.\nNotice that as a result of the kernel/memory overlap the average device time is no longer equal to the average amount of time to process a frame on the device (streamed device time). In fact because of kernel/memory and host/device overlap the average device time should now be greater than both the average streamed device and frame time.\n\n\n\n\n\n\n\n\n\nHypothesis\n\n\n\n\n\nIf the above assumption is correct we should be able to see this effect by using device timers to get a more accurate value for the average device time.\n\n\n\n\n\n\n\n\n\nNext\n\n\n\n\n\n\nUse device timers to get the average device time. Unfortunately this introduces some overhead so we will have to compare this to the average time required to process each frame calculated without the device timers. This may mean that we may not see the difference that we expect.\nCalculate the theoretical average time to process each frame on the host and then the device without overlap (host time + device time), to see the gain from host/device and kernel/memory overlap.\nCalculate the average wasted time on the host (streamed device time - host time) time where the host could be performing useful operations without increasing the average processing time).",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/opencv_cuda_streams_performance_python.html#analysis-7",
    "href": "nbs/opencv_cuda_streams_performance_python.html#analysis-7",
    "title": "Accelerating OpenCV with Python and CUDA streams",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nIt appears that we gained 0.04 ms/frame from the kernel/memory processing overlap on the device. Unfortunately we cannot say this for sure because the times compared here are from two separate runs due to the device timer overhead. That said, the implication is that our interpretation of the kernel/memory overlap seen in the Nvidia Visual Profiler is correct.\nThe total processing which needs to be performed on the host and device takes and average of 1.71 ms/frame which is 0.74 ms/frame greater than our final implementation, demonstrating the importance of using asynchronous device calls and CUDA streams.\nWe have 0.26 ms/frame to spare on the host which we can make use of without affecting the average frame time of 0.97 ms/frame.",
    "crumbs": [
      "OpenCV",
      "Accelerate with CUDA streams in Python"
    ]
  },
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html",
    "title": "ImportError: DLL load failed while importing cv2: The specified module could not be found.",
    "section": "",
    "text": "If you’re using Windows with Python &gt;= 3.8, have built OpenCV &gt;= 4.6 from source, and are encountering the “ImportError: DLL load failed while importing cv2: The specified module could not be found” error when calling import cv2, this short guide should help solve your problem.\nThis guide assumes that you have either installed the Python bindings during the build process or manually copied cv2.cpxx-win_amd64.pyd to your distribution’s site-packages directory (e.g., C:\\Users\\&lt;USER&gt;\\miniforge3\\Lib\\site-packages).\nSo, what’s the issue? Although the error message is quite explicit about the cause, it doesn’t really help with finding a solution. In a nutshell, Python has found cv2.cpxx-win_amd64.pyd tried to load it, and then failed because it can’t find a dependent shared library.\nThe advice I’ve seen online regarding this issue is to use Dependency Walker, load the cv2.cpxx-win_amd64.pyd, and identify which dependencies the system can’t find. This is solid advice if you have a C++ application, are using Python &lt; 3.8 (which uses the system/user path for DLL resolution), or are not using a Python distribution that performs path manipulation under the hood (e.g., Anaconda). However, if the above does not apply, even if Dependency Walker doesn’t detect any problems, we may still face the above error.\nThe good news is that there’s an easy fix if you know where the missing DLLs are, and it’s only slightly more involved if you don’t, as long as you have access to the missing DLLs on your system.",
    "crumbs": [
      "OpenCV",
      "ImportError: DLL load failed..."
    ]
  },
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#manually-adding-filter-entries",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#manually-adding-filter-entries",
    "title": "ImportError: DLL load failed while importing cv2: The specified module could not be found.",
    "section": "Manually Adding Filter Entries",
    "text": "Manually Adding Filter Entries\nSince we only want to view shared libraries accessed by the python.exe process, we can add the following filters to make our task easier:\n\nProcess Name -&gt; is -&gt; python.exe\nOperation -&gt; is -&gt; CreateFile\nResult -&gt; is -&gt; NAME NOT FOUND\nResult -&gt; is -&gt; SUCCESS\nPath -&gt; contains -&gt; .dll\nPath -&gt; contains -&gt; .pyd (not striclty necessary, if this was missing the error would be “ModuleNotFoundError: No module named ‘cv2’” however for completeness we’ll include it)\n\nYour filter should now resemble the one in the screenshot below:\n\n\n\nProcess Monitor Filter",
    "crumbs": [
      "OpenCV",
      "ImportError: DLL load failed..."
    ]
  },
  {
    "objectID": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#using-process-monitor-to-find-the-missin-dlls",
    "href": "nbs/ImportError_dll_load_failed_while_importing_cv2.html#using-process-monitor-to-find-the-missin-dlls",
    "title": "ImportError: DLL load failed while importing cv2: The specified module could not be found.",
    "section": "Using Process Monitor to find the missin DLLs",
    "text": "Using Process Monitor to find the missin DLLs\nBefore continuing, it is advisable to close any other Python processes, as the output from these will pollute the main window of Process Monitor.\nNow, follow these steps:\n\nStart Python.\nPress the clear button (red trash can icon) in Process Monitor to clear any output generated during Python’s initialization.\nImport OpenCV (import cv2)\n\nTo demonstrate how this works I have restarted Pyton to reset the paths we manually added with the above calls to os.add_dll_directory() and again run import cv2, resulting in the same error with the additional output from Process Monitor shown in the screen shot below:\n\nimport cv2\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 import cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\n\nProcess Monitor failed to locate opencv_world4100.dll and opencv_img_hash_4100.dll\n\n\nThis output shows that we successfully found cv2.cp310-win_amd64.pyd (otherwise we would see the “ModuleNotFoundError: No module named ‘cv2’” error). However, it also reveals several unsuccessful attempts to locate opencv_world4100.dll and opencv_img_hash_4100.dll in different directories.\n\n\n\n\n\n\nNote\n\n\n\n\n\nBecause there is a lot of output to sift through, if you can’t immediately see which DLLs are missing, I recommend exporting and parsing the output from Process Monitor as described in the “Automatically search Process Monitor log for missing DLL” section below.\n\n\n\nAs before, we need to add the directory containing these missing DLLs to the Python DLL search path shown below:\n\nimport os\nos.add_dll_directory(r'D:\\build\\opencv\\4_10\\install\\x64\\vc17\\bin')\nimport cv2\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[2], line 3\n      1 import os\n      2 os.add_dll_directory(r'D:\\build\\opencv\\4_10\\install\\x64\\vc17\\bin')\n----&gt; 3 import cv2\n\nImportError: DLL load failed while importing cv2: The specified module could not be found.\n\n\n\n\n\n\nProcess Monitor failed to locate nppial64_12.dll and nppc64_12.dll\n\n\nAfter adding the previously missing DLLs to the search path, Process Monitor now shows that opencv_world4100.dll and opencv_img_hash_4100.dll were located successfully after a few attempts. However, we’re now missing nppial64_12.dll and nppc64_12.dll, which are part of the CUDA SDK. To resolve this, we need to add the CUDA SDK binary directory to the Python DLL search path. Once we do this, the call to import cv2 will be successful.\nIf we were still seeing the same error after this step, we would simply repeat the process:\n\nExamine the output in Process Monitor\nIdentify any remaining missing DLLs\nAdd the directories containing these missing DLLs to the Python DLL search path",
    "crumbs": [
      "OpenCV",
      "ImportError: DLL load failed..."
    ]
  },
  {
    "objectID": "qmd/about.html",
    "href": "qmd/about.html",
    "title": "About",
    "section": "",
    "text": "Location for storing useful guides and notebooks\n\n\n Back to top"
  },
  {
    "objectID": "qmd/opencv_cuda_performance.html",
    "href": "qmd/opencv_cuda_performance.html",
    "title": "OpenCV CUDA Performance Comparison (Nvidia vs Intel)",
    "section": "",
    "text": "In this post I am going to use the OpenCV’s performance tests to compare the CUDA and CPU implementations. The idea, is to get an indication of which OpenCV and/or Computer Vision algorithms, in general, benefit the most from GPU acceleration, and therefore, under what circumstances it might be a good idea to invest in a GPU.",
    "crumbs": [
      "OpenCV",
      "CUDA Performance Comparisson"
    ]
  },
  {
    "objectID": "qmd/opencv_cuda_performance.html#gpu-specifications",
    "href": "qmd/opencv_cuda_performance.html#gpu-specifications",
    "title": "OpenCV CUDA Performance Comparison (Nvidia vs Intel)",
    "section": "GPU specifications",
    "text": "GPU specifications\nThe GPU’s tested comprise three different micro-architectures, ranging from a low end laptop (730m) to a mid range desktop (GTX 1060) GPU. The full specifications are shown below, where I have also included the maximum theoretical speedup, if the OpenCV function were bandwidth or compute limited. This value is just included to give an indication of what should be possible if architectural improvements, SM count etc. don’t have any impact on performance. In “general” most algorithms will be bandwidth limited implying that the average speed up of the OpenCV functions could be somewhere between these two values. If you are not familiar with this concept then I would recommend watching Memory Bandwidth Bootcamp: Best Practices, Memory Bandwidth Bootcamp: Beyond Best Practices and Memory Bandwidth Bootcamp: Collaborative Access Patterns by Tony Scudiero for a good overview.",
    "crumbs": [
      "OpenCV",
      "CUDA Performance Comparisson"
    ]
  }
]